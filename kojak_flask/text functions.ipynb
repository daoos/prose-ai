{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:43.641444Z",
     "start_time": "2017-09-19T00:05:43.636707Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T22:40:23.448248Z",
     "start_time": "2017-09-12T22:40:23.437329Z"
    }
   },
   "source": [
    "## Quill stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:43.650161Z",
     "start_time": "2017-09-19T00:05:43.645912Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_delta = '''{\"ops\":[\n",
    "  { \"insert\": \"#\" },\n",
    "  { \"insert\": \"!\", \"attributes\": { \"bold\": \"true\" }}\n",
    "]}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:43.690226Z",
     "start_time": "2017-09-19T00:05:43.667262Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "!\n",
      "{'bold': 'true'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ops': [{'insert': '#'}, {'attributes': {'bold': 'true'}, 'insert': '!'}]}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "def read_Delta(Delta):\n",
    "    '''Expects a list of operations (dicts)'''\n",
    "    if type(Delta) == str:\n",
    "        Delta = json.loads(Delta)\n",
    "    \n",
    "    for op_dict in Delta[\"ops\"]:\n",
    "        for op, value in op_dict.items():\n",
    "            print(value)\n",
    "            if (op == 'insert') and (value==\"#\"): \n",
    "                hashtag = True\n",
    "    return Delta\n",
    "\n",
    "read_Delta(test_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy and TextaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:50.825535Z",
     "start_time": "2017-09-19T00:05:43.696005Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def to_spacy_doc(raw_doc):\n",
    "    '''Converts a raw string into a spaCy document'''\n",
    "    return nlp(raw_doc)\n",
    "\n",
    "def to_textacy_doc(raw_doc):\n",
    "    '''Converts a raw string into a spaCy doc, then a textacy doc'''\n",
    "    if isinstance(to_spacy_doc(\"test\"), spacy.tokens.doc.Doc):\n",
    "        return textacy.Doc(raw_doc)\n",
    "    else:\n",
    "        return textacy.Doc(nlp(raw_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample input documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:52.160310Z",
     "start_time": "2017-09-19T00:05:50.836824Z"
    }
   },
   "outputs": [],
   "source": [
    "### Setup testing documents\n",
    "doc_path = 'corpus_data/comp_ling.txt'\n",
    "\n",
    "with open(doc_path) as f:\n",
    "    input_doc = f.read()\n",
    "\n",
    "doc=to_spacy_doc(input_doc)\n",
    "\n",
    "autism_abstract = '''The purpose of this research is to identify a subtype of autism called Developmental Verbal Dyspraxia (DVD).  DVD is a motor-speech problem, disabling oral-motor movements needed for speaking. The first phase of the project involves a screening interview where we identify DVD and Non-DVD kids.  We also use home videos to validate answers on the screening interview.  The final phase involves home visits where we use several assessments to confirm the child’s diagnosis and examine the connection between manual and oral motor challenges. By identifying DVD as a subtype of Autism, we will eliminate the assumption that all Autistics have the same characteristics. This will allow for more individual consideration of Autistic people and may direct future research on the genetic factors in autism.'''\n",
    "history_abstract = '''This project involves discovering how the American Revolution was remembered during the nineteenth century.  The goal is to show that the American Revolution was memorialized by the actions of the United States government during the 1800s. This has been done by examining events such as the Supreme Court cases of John Marshall and the Nullification Crisis. Upon examination of these events, it becomes clear that John Marshall and John Calhoun (creator of the Doctrine of Nullification) attempted to use the American Revolution to bolster their claims by citing speeches from Founding Fathers. Through showing that the American Revolution lives on in memory, this research highlights the importance of the revolution in shaping the actions of the United States government.'''\n",
    "games_abstract = '''The study is to show how even a “sport” video game can incorporate many types of learning, to call attention to what might be overlooked as significant forms of learning, and to understand and take advantage of the opportunities video games afford as more deliberate learning environments. The aspects explored are the skills and techniques required to be successful in the game, the environment that skaters skate in, the personal vs. group identity that is shown through the general appearance of the skater, and the values and icons that the game teaches players. We are finding that sport video games support learning; we hope to find how one learns about oneself as a learner from playing.'''\n",
    "\n",
    "raw_abstracts = [autism_abstract, history_abstract, games_abstract]\n",
    "abstracts = list(map(to_textacy_doc, raw_abstracts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy parse as Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:52.408269Z",
     "start_time": "2017-09-19T00:05:52.163616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computational</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>-4.329765</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an</td>\n",
       "      <td>-5.953294</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>interdisciplinary</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>field</td>\n",
       "      <td>-9.710699</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>concerned</td>\n",
       "      <td>-9.861534</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>with</td>\n",
       "      <td>-5.363765</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>statistical</td>\n",
       "      <td>-11.639928</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>or</td>\n",
       "      <td>-5.715355</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rule</td>\n",
       "      <td>-9.545105</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-</td>\n",
       "      <td>-5.202416</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>based</td>\n",
       "      <td>-8.318480</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>modeling</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>of</td>\n",
       "      <td>-4.128464</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>natural</td>\n",
       "      <td>-9.312656</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>language</td>\n",
       "      <td>-8.811478</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>from</td>\n",
       "      <td>-6.028811</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a</td>\n",
       "      <td>-3.983075</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>computational</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>perspective</td>\n",
       "      <td>-9.924529</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.391480</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>as</td>\n",
       "      <td>-5.507394</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>well</td>\n",
       "      <td>-7.117937</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>as</td>\n",
       "      <td>-5.507394</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>study</td>\n",
       "      <td>-9.603720</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>of</td>\n",
       "      <td>-4.128464</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>appropriate</td>\n",
       "      <td>-10.032200</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>finding</td>\n",
       "      <td>-9.960837</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>pictures</td>\n",
       "      <td>-9.748672</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>of</td>\n",
       "      <td>-4.128464</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>a</td>\n",
       "      <td>-3.983075</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>red</td>\n",
       "      <td>-9.453293</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>truck</td>\n",
       "      <td>-10.898046</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.391480</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>search</td>\n",
       "      <td>-9.428295</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>engine</td>\n",
       "      <td>-10.291152</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>will</td>\n",
       "      <td>-6.105684</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3736</th>\n",
       "      <td>still</td>\n",
       "      <td>-7.043760</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>find</td>\n",
       "      <td>-7.562676</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>information</td>\n",
       "      <td>-8.817930</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>desired</td>\n",
       "      <td>-11.827469</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>by</td>\n",
       "      <td>-6.114920</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>matching</td>\n",
       "      <td>-11.781653</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3743</th>\n",
       "      <td>words</td>\n",
       "      <td>-8.599854</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3744</th>\n",
       "      <td>such</td>\n",
       "      <td>-7.619059</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>as</td>\n",
       "      <td>-5.507394</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>\"</td>\n",
       "      <td>-4.700859</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>four</td>\n",
       "      <td>-9.416868</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>-</td>\n",
       "      <td>-5.202416</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>wheeled</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>\"</td>\n",
       "      <td>-4.700859</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>with</td>\n",
       "      <td>-5.363765</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>\"</td>\n",
       "      <td>-4.700859</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>car\".[37</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>]</td>\n",
       "      <td>-5.498423</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3755 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   text  log_probability stop? punctuation? whitespace?  \\\n",
       "0         Computational       -19.579313                                  \n",
       "1           linguistics       -19.579313                                  \n",
       "2                    is        -4.329765   Yes                            \n",
       "3                    an        -5.953294   Yes                            \n",
       "4     interdisciplinary       -19.579313                                  \n",
       "5                 field        -9.710699                                  \n",
       "6             concerned        -9.861534                                  \n",
       "7                  with        -5.363765   Yes                            \n",
       "8                   the        -3.425446   Yes                            \n",
       "9           statistical       -11.639928                                  \n",
       "10                   or        -5.715355   Yes                            \n",
       "11                 rule        -9.545105                                  \n",
       "12                    -        -5.202416                Yes               \n",
       "13                based        -8.318480                                  \n",
       "14             modeling       -19.579313                                  \n",
       "15                   of        -4.128464   Yes                            \n",
       "16              natural        -9.312656                                  \n",
       "17             language        -8.811478                                  \n",
       "18                 from        -6.028811   Yes                            \n",
       "19                    a        -3.983075   Yes                            \n",
       "20        computational       -19.579313                                  \n",
       "21          perspective        -9.924529                                  \n",
       "22                    ,        -3.391480                Yes               \n",
       "23                   as        -5.507394   Yes                            \n",
       "24                 well        -7.117937   Yes                            \n",
       "25                   as        -5.507394   Yes                            \n",
       "26                  the        -3.425446   Yes                            \n",
       "27                study        -9.603720                                  \n",
       "28                   of        -4.128464   Yes                            \n",
       "29          appropriate       -10.032200                                  \n",
       "...                 ...              ...   ...          ...         ...   \n",
       "3725            finding        -9.960837                                  \n",
       "3726           pictures        -9.748672                                  \n",
       "3727                 of        -4.128464   Yes                            \n",
       "3728                  a        -3.983075   Yes                            \n",
       "3729                red        -9.453293                                  \n",
       "3730              truck       -10.898046                                  \n",
       "3731                  ,        -3.391480                Yes               \n",
       "3732                the        -3.425446   Yes                            \n",
       "3733             search        -9.428295                                  \n",
       "3734             engine       -10.291152                                  \n",
       "3735               will        -6.105684   Yes                            \n",
       "3736              still        -7.043760   Yes                            \n",
       "3737               find        -7.562676                                  \n",
       "3738                the        -3.425446   Yes                            \n",
       "3739        information        -8.817930                                  \n",
       "3740            desired       -11.827469                                  \n",
       "3741                 by        -6.114920   Yes                            \n",
       "3742           matching       -11.781653                                  \n",
       "3743              words        -8.599854                                  \n",
       "3744               such        -7.619059   Yes                            \n",
       "3745                 as        -5.507394   Yes                            \n",
       "3746                  \"        -4.700859                Yes               \n",
       "3747               four        -9.416868   Yes                            \n",
       "3748                  -        -5.202416                Yes               \n",
       "3749            wheeled       -19.579313                                  \n",
       "3750                  \"        -4.700859                Yes               \n",
       "3751               with        -5.363765   Yes                            \n",
       "3752                  \"        -4.700859                Yes               \n",
       "3753           car\".[37       -19.579313                                  \n",
       "3754                  ]        -5.498423                Yes               \n",
       "\n",
       "     number? out of vocab.?  \n",
       "0                            \n",
       "1                            \n",
       "2                            \n",
       "3                            \n",
       "4                            \n",
       "5                            \n",
       "6                            \n",
       "7                            \n",
       "8                            \n",
       "9                            \n",
       "10                           \n",
       "11                           \n",
       "12                           \n",
       "13                           \n",
       "14                           \n",
       "15                           \n",
       "16                           \n",
       "17                           \n",
       "18                           \n",
       "19                           \n",
       "20                           \n",
       "21                           \n",
       "22                           \n",
       "23                           \n",
       "24                           \n",
       "25                           \n",
       "26                           \n",
       "27                           \n",
       "28                           \n",
       "29                           \n",
       "...      ...            ...  \n",
       "3725                         \n",
       "3726                         \n",
       "3727                         \n",
       "3728                         \n",
       "3729                         \n",
       "3730                         \n",
       "3731                         \n",
       "3732                         \n",
       "3733                         \n",
       "3734                         \n",
       "3735                         \n",
       "3736                         \n",
       "3737                         \n",
       "3738                         \n",
       "3739                         \n",
       "3740                         \n",
       "3741                         \n",
       "3742                         \n",
       "3743                         \n",
       "3744                         \n",
       "3745                         \n",
       "3746                         \n",
       "3747     Yes                 \n",
       "3748                         \n",
       "3749                         \n",
       "3750                         \n",
       "3751                         \n",
       "3752                         \n",
       "3753                    Yes  \n",
       "3754                         \n",
       "\n",
       "[3755 rows x 7 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### view how spacy parses doc\n",
    "import pandas as pd\n",
    "\n",
    "token_attributes = [(token.orth_,\n",
    "                     token.prob,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num,\n",
    "                     token.is_oov)\n",
    "                    for token in doc]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'log_probability',\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           'out of vocab.?'])\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Vector Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:46:04.799673Z",
     "start_time": "2017-09-19T00:46:03.631080Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['information',\n",
       " 'data',\n",
       " 'feedback',\n",
       " 'publicity',\n",
       " 'airflow',\n",
       " 'SELF-ANALYSIS',\n",
       " 'self-analysis',\n",
       " 'ribbing',\n",
       " 'WISECRACKS',\n",
       " 'wisecracks',\n",
       " 'Matter-Of-Factness',\n",
       " 'matter-of-factness',\n",
       " 'Balustrade',\n",
       " 'balustrade',\n",
       " 'retrievable',\n",
       " 'Birdies',\n",
       " 'birdies',\n",
       " 'humidifier',\n",
       " 'RIBBING',\n",
       " 'DATA-CAPTURE',\n",
       " 'data-capture',\n",
       " 'Kvetching',\n",
       " 'kvetching',\n",
       " 'Refreshment',\n",
       " 'refreshment',\n",
       " 'pagination',\n",
       " 'PAGINATION',\n",
       " 'brassiere',\n",
       " 'MATTER-OF-FACTNESS',\n",
       " 'sonnets',\n",
       " 'Discernment',\n",
       " 'discernment',\n",
       " 'rehabilitative',\n",
       " 'Soot',\n",
       " 'soot',\n",
       " 'guidance',\n",
       " 'HUMIDIFIER',\n",
       " 'COUNTRY-DIRECT',\n",
       " 'country-direct',\n",
       " 'dimples',\n",
       " 'DISCERNMENT',\n",
       " 'BRASSIERE',\n",
       " 'sublimits',\n",
       " 'Pagination',\n",
       " 'headsets',\n",
       " 'primers',\n",
       " 'Humidifier',\n",
       " 'PAYABLES',\n",
       " 'payables',\n",
       " 'rights-of-way',\n",
       " 'BIRDIES',\n",
       " 'Dimples',\n",
       " 'alarm',\n",
       " 'eavesdropping',\n",
       " 'TELEGRAMS',\n",
       " 'telegrams',\n",
       " 'SOOT',\n",
       " 'counter-measures',\n",
       " 'Country-Direct',\n",
       " 'BALUSTRADE',\n",
       " 'KVETCHING',\n",
       " 'Retrievable',\n",
       " 'NEWS-RETRIEVAL',\n",
       " 'news-retrieval',\n",
       " 'FISH-SHOOTING',\n",
       " 'fish-shooting',\n",
       " 'torque',\n",
       " 'Sublimits',\n",
       " 'Counter-Measures',\n",
       " 'Self-Analysis',\n",
       " 'Primers',\n",
       " 'reservations',\n",
       " 'moralizing',\n",
       " 'Data-Capture',\n",
       " 'Ribbing',\n",
       " 'News-Retrieval',\n",
       " 'Overbillings',\n",
       " 'overbillings',\n",
       " 'Fish-Shooting',\n",
       " 'DIMPLES',\n",
       " 'Rights-Of-Way',\n",
       " 'Headsets',\n",
       " 'HEADSETS',\n",
       " 'REFRESHMENT',\n",
       " 'TORQUE',\n",
       " 'Wisecracks',\n",
       " 'REHABILITATIVE',\n",
       " 'RETRIEVABLE',\n",
       " 'Brassiere',\n",
       " 'SUBLIMITS',\n",
       " 'OVERBILLINGS',\n",
       " 'PRIMERS',\n",
       " 'RIGHTS-OF-WAY',\n",
       " 'COUNTER-MEASURES',\n",
       " 'Payables']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def similar_to_target_by_brown_cluster(words_to_compare, target_word):\n",
    "    similar_words = []\n",
    "    for word_to_compare in words_to_compare:\n",
    "        if abs(nlp.vocab[word_to_compare].cluster - nlp.vocab[target_word].cluster) < 1:\n",
    "            similar_words.append(word_to_compare)\n",
    "    return similar_words\n",
    "\n",
    "similar_to_target_by_brown_cluster(list(nlp.vocab.strings), 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:52.434091Z",
     "start_time": "2017-09-19T00:05:52.426834Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = input_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:54.822288Z",
     "start_time": "2017-09-19T00:05:52.437284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Computational, linguistics, interdisciplinary, field, concerned, statistical, rule, based, modeling, natural, language, computational, perspective, study, appropriate, computational, approaches, linguistic, questions, Traditionally, computational, linguistics, performed, computer, scientists, specialized, application, computers, processing, natural, language, Today, computational, linguists, work, members, interdisciplinary, teams, include, regular, linguists, experts, target, language, computer, scientists, general, computational, linguistics, draws, involvement, linguists, computer, scientists, experts, artificial, intelligence, mathematicians, logicians, philosophers, cognitive, scientists, cognitive, psychologists, psycholinguists, anthropologists, neuroscientists, Computational, linguistics, theoretical, applied, components, Theoretical, computational, linguistics, focuses, issues, theoretical, linguistics, cognitive, science, applied, computational, linguistics, focuses, practical, outcome, modeling, human, language, use.[1, Association, Computational, Linguistics, defines, computational, linguistics, scientific, study, language, computational, perspective, Computational, linguists, interested, providing, computational, models, kinds, linguistic, phenomena.[2, Contents, hide, 1, Origins, 2, Approaches, 2.1, Developmental, approaches, 2.2, Structural, approaches, 2.3, Production, approaches, 2.3.1, Text, based, interactive, approach, 2.3.2, Speech, based, interactive, approach, 2.4, Comprehension, approaches, 3, Applications, 4, Subfields, 5, Legacy, 6, 7, References, 8, External, links, Origins[edit, Computational, linguistics, grouped, field, artificial, intelligence, actually, present, development, artificial, intelligence, Computational, linguistics, originated, efforts, United, States, 1950s, use, computers, automatically, translate, texts, foreign, languages, particularly, Russian, scientific, journals, English.[3, computers, arithmetic, calculations, faster, accurately, humans, thought, short, matter, time, begin, process, language.[4, Computational, quantitative, methods, historically, attempted, reconstruction, earlier, forms, modern, languages, subgrouping, modern, languages, language, families, Earlier, methods, lexicostatistics, glottochronology, proven, premature, inaccurate, recent, interdisciplinary, studies, borrow, concepts, biological, studies, especially, gene, mapping, proved, produce, sophisticated, analytical, tools, trustful, results.[5, machine, translation, known, mechanical, translation, failed, yield, accurate, translations, right, away, automated, processing, human, languages, recognized, far, complex, originally, assumed, Computational, linguistics, born, new, field, study, devoted, developing, algorithms, software, intelligently, processing, language, data, artificial, intelligence, came, existence, 1960s, field, computational, linguistics, sub, division, artificial, intelligence, dealing, human, level, comprehension, production, natural, languages.[citation, needed, order, translate, language, observed, understand, grammar, languages, including, morphology, grammar, word, forms, syntax, grammar, sentence, structure, order, understand, syntax, understand, semantics, lexicon, vocabulary, pragmatics, language, use, started, effort, translate, languages, evolved, entire, discipline, devoted, understanding, represent, process, natural, languages, computers.[6, Nowadays, research, scope, computational, linguistics, computational, linguistics, departments,[7, computational, linguistics, laboratories,[8, computer, science, departments,[9, linguistics, departments.[10][11, research, field, computational, linguistics, aims, create, working, speech, text, processing, systems, aim, create, system, allowing, human, machine, interaction, Programs, meant, human, machine, communication, called, conversational, agents.[12, Approaches[edit, computational, linguistics, performed, experts, variety, fields, wide, assortment, departments, research, fields, broach, diverse, range, topics, following, sections, discuss, literature, available, entire, field, broken, main, area, discourse, developmental, linguistics, structural, linguistics, linguistic, production, linguistic, comprehension, Developmental, approaches[edit, Language, cognitive, skill, develops, life, individual, developmental, process, examined, number, techniques, computational, approach, Human, language, development, provide, constraints, harder, apply, computational, method, understanding, instance, language, acquisition, human, children, largely, exposed, positive, evidence.[13, means, linguistic, development, individual, evidence, correct, form, provided, evidence, correct, insufficient, information, simple, hypothesis, testing, procedure, information, complex, language,[14, provides, certain, boundaries, computational, approach, modeling, language, development, acquisition, individual, Attempts, model, developmental, process, language, acquisition, children, computational, angle, leading, statistical, grammars, connectionist, models.[15, Work, realm, proposed, method, explain, evolution, language, history, models, shown, languages, learned, combination, simple, input, presented, incrementally, child, develops, better, memory, longer, attention, span.[16, simultaneously, posed, reason, long, developmental, period, human, children.[16, conclusions, drawn, strength, neural, network, project, created, ability, infants, develop, language, modeled, robots[17, order, test, linguistic, theories, Enabled, learn, children, model, created, based, affordance, model, mappings, actions, perceptions, effects, created, linked, spoken, words, Crucially, robots, able, acquire, functioning, word, meaning, mappings, needing, grammatical, structure, vastly, simplifying, learning, process, shedding, light, information, furthers, current, understanding, linguistic, development, important, note, information, empirically, tested, computational, approach, understanding, linguistic, development, individual, lifetime, continually, improved, neural, networks, learning, robotic, systems, important, mind, languages, change, develop, time, Computational, approaches, understanding, phenomenon, unearthed, interesting, information, Price, Equation, Pólya, urn, dynamics, researchers, created, system, predicts, future, linguistic, evolution, gives, insight, evolutionary, history, modern, day, languages.[18, modeling, effort, achieved, computational, linguistics, impossible, clear, understanding, linguistic, development, humans, evolutionary, time, fantastically, improved, advances, computational, linguistics, ability, model, modify, systems, affords, science, ethical, method, testing, hypotheses, intractable, Structural, approaches[edit, order, create, better, computational, models, language, understanding, language, ’s, structure, crucial, end, English, language, meticulously, studied, computational, approaches, better, understand, language, works, structural, level, important, pieces, able, study, linguistic, structure, availability, large, linguistic, corpora, samples, grants, computational, linguists, raw, data, necessary, run, models, gain, better, understanding, underlying, structures, present, vast, data, contained, single, language, cited, English, linguistic, corpora, Penn, Treebank.[19, Derived, widely, different, sources, IBM, computer, manuals, transcribed, telephone, conversations, corpus, contains, 4.5, million, words, American, English, corpus, primarily, annotated, speech, tagging, syntactic, bracketing, yielded, substantial, empirical, observations, related, language, structure.[20, Theoretical, approaches, structure, languages, developed, works, allow, computational, linguistics, framework, work, hypotheses, understanding, language, myriad, ways, original, theoretical, theses, internalization, grammar, structure, language, proposed, types, models.[14, models, rules, patterns, learned, increase, strength, frequency, encounter.[14, work, created, question, computational, linguists, answer, infant, learn, specific, non, normal, grammar, Chomsky, Normal, Form, learning, overgeneralized, version, getting, stuck?[14, Theoretical, efforts, like, set, direction, research, early, lifetime, field, study, crucial, growth, field, Structural, information, languages, allows, discovery, implementation, similarity, recognition, pairs, text, utterances.[21, instance, recently, proven, based, structural, information, present, patterns, human, discourse, conceptual, recurrence, plots, model, visualize, trends, data, create, reliable, measures, similarity, natural, textual, utterances.[21, technique, strong, tool, probing, structure, human, discourse, computational, approach, question, vastly, complex, information, present, discourse, data, remained, inaccessible, scientists, Information, structural, data, language, available, English, languages, Japanese.[22, computational, methods, Japanese, sentence, corpora, analyzed, pattern, log, normality, found, relation, sentence, length.[22, exact, cause, lognormality, remains, unknown, precisely, sort, intriguing, information, computational, linguistics, designed, uncover, information, lead, important, discoveries, underlying, structure, Japanese, number, effects, understanding, Japanese, language, Computational, linguistics, allows, exciting, additions, scientific, knowledge, base, happen, quickly, little, room, doubt, computational, approach, structure, linguistic, data, information, available, hidden, vastness, data, single, language, Computational, linguistics, allows, scientists, parse, huge, amounts, data, reliably, efficiently, creating, possibility, discoveries, unlike, seen, approaches, Production, approaches[edit, section, possibly, contains, original, research, improve, verifying, claims, adding, inline, citations, Statements, consisting, original, research, removed, October, 2015, Learn, remove, template, message, production, language, equally, complex, information, provides, necessary, skills, fluent, producer, comprehension, half, problem, communication, half, system, produces, language, computational, linguistics, interesting, discoveries, area, Alan, Turing, computer, scientist, namesake, developer, Turing, Test, method, measuring, intelligence, machine, famous, paper, published, 1950, Alan, Turing, proposed, possibility, machines, day, ability, think, thought, experiment, define, concept, thought, machines, proposed, imitation, test, human, subject, text, conversations, fellow, human, machine, attempting, respond, like, human, Turing, proposes, subject, tell, difference, human, machine, concluded, machine, capable, thought.[23, Today, test, known, Turing, test, remains, influential, idea, area, artificial, intelligence, Joseph, Weizenbaum, MIT, professor, computer, scientist, developed, ELIZA, primitive, computer, program, utilizing, natural, language, processing, earliest, best, known, examples, computer, program, designed, converse, naturally, humans, ELIZA, program, developed, Joseph, Weizenbaum, MIT, 1966, program, emulated, Rogerian, psychotherapist, responding, written, statements, questions, posed, user, appeared, capable, understanding, said, responding, intelligently, truth, simply, followed, pattern, matching, routine, relied, understanding, keywords, sentence, responses, generated, recombining, unknown, parts, sentence, properly, translated, versions, known, words, example, phrase, hate, ELIZA, understands, matches, general, pattern, words, allowing, ELIZA, update, words, replying, makes, think, hate, example, ELIZA, understanding, word, hate, required, logical, response, context, type, psychotherapy.[24, projects, trying, solve, problem, started, computational, linguistics, field, place, methods, refined, clever, consequently, results, generated, computational, linguists, enlightening, effort, improve, computer, translation, models, compared, including, hidden, Markov, models, smoothing, techniques, specific, refinements, apply, verb, translation.[25, model, found, produce, natural, translations, German, French, words, refined, alignment, model, order, dependence, fertility, model[16, provide, efficient, training, algorithms, models, presented, scientists, ability, improve, results, type, work, specific, computational, linguistics, applications, vastly, improve, understanding, language, produced, comprehended, computers, Work, making, computers, produce, language, naturalistic, manner, linguistic, input, humans, algorithms, constructed, able, modify, system, 's, style, production, based, factor, linguistic, input, human, abstract, factors, like, politeness, main, dimensions, personality.[26, work, takes, computational, approach, parameter, estimation, models, categorize, vast, array, linguistic, styles, individuals, simplify, computer, work, way, making, human, computer, interaction, natural, Text, based, interactive, approach[edit, earliest, simplest, models, human, computer, interaction, ELIZA, example, involve, text, based, input, user, generate, response, computer, method, words, typed, user, trigger, computer, recognize, specific, patterns, reply, accordingly, process, known, keyword, spotting, Speech, based, interactive, approach[edit, Recent, technologies, placed, emphasis, speech, based, interactive, systems, systems, Siri, iOS, operating, system, operate, similar, pattern, recognizing, technique, text, based, systems, user, input, conducted, speech, recognition, branch, linguistics, involves, processing, user, 's, speech, sound, waves, interpreting, acoustics, language, patterns, order, computer, recognize, input.[27, Comprehension, approaches[edit, focus, modern, computational, linguistics, comprehension, proliferation, internet, abundance, easily, accessible, written, human, language, ability, create, program, capable, understanding, human, language, broad, exciting, possibilities, including, improved, search, engines, automated, customer, service, online, education, Early, work, comprehension, included, applying, Bayesian, statistics, task, optical, character, recognition, illustrated, Bledsoe, Browing, 1959, large, dictionary, possible, letters, generated, learning, example, letters, probability, learned, examples, matched, new, input, combined, final, decision.[28, attempts, applying, Bayesian, statistics, language, analysis, included, work, Mosteller, Wallace, 1963, analysis, words, Federalist, Papers, attempt, determine, authorship, concluding, Madison, likely, authored, majority, papers).[29, 1971, Terry, Winograd, developed, early, natural, language, processing, engine, capable, interpreting, naturally, written, commands, simple, rule, governed, environment, primary, language, parsing, program, project, called, SHRDLU, capable, carrying, somewhat, natural, conversation, user, giving, commands, scope, toy, environment, designed, task, environment, consisted, different, shaped, colored, blocks, SHRDLU, capable, interpreting, commands, Find, block, taller, holding, box, asking, questions, n't, understand, pyramid, mean, response, user, 's, input.[30, impressive, kind, natural, language, processing, proven, difficult, outside, limited, scope, toy, environment, Similarly, project, developed, NASA, called, LUNAR, designed, provide, answers, naturally, written, questions, geological, analysis, lunar, rocks, returned, Apollo, missions.[31, kinds, problems, referred, question, answering, Initial, attempts, understanding, spoken, language, based, work, 1960s, 1970s, signal, modeling, unknown, signal, analyzed, look, patterns, predictions, based, history, initial, somewhat, successful, approach, applying, kind, signal, modeling, language, achieved, use, hidden, Markov, models, detailed, Rabiner, 1989.[32, approach, attempts, determine, probabilities, arbitrary, number, models, generating, speech, modeling, probabilities, words, generated, possible, models, Similar, approaches, employed, early, speech, recognition, attempts, starting, late, 70s, IBM, word, speech, pair, probabilities.[33, recently, kinds, statistical, approaches, applied, difficult, tasks, topic, identification, Bayesian, parameter, estimation, infer, topic, probabilities, text, documents.[34, Applications[edit, Modern, computational, linguistics, combination, studies, computer, science, programming, math, particularly, statistics, language, structures, natural, language, processing, Combined, fields, lead, development, systems, recognize, speech, perform, task, based, speech, Examples, include, speech, recognition, software, Apple, 's, Siri, feature, spellcheck, tools, speech, synthesis, programs, demonstrate, pronunciation, help, disabled, machine, translation, programs, websites, Google, Translate, Word, Reference.[35, Computational, linguistics, especially, helpful, situations, involving, social, media, Internet, example, filters, chatrooms, website, searches, require, computational, linguistics, Chat, operators, use, filters, identify, certain, words, phrases, deem, inappropriate, users, submit, them.[35, example, filters, websites, Schools, use, filters, websites, certain, keywords, blocked, children, view, programs, parents, use, Parental, controls, content, filters, place, Computational, linguists, develop, programs, group, organize, content, Social, media, mining, example, Twitter, programs, group, tweets, subject, keywords.[36, Computational, linguistics, document, retrieval, clustering, online, search, documents, websites, retrieved, based, frequency, unique, labels, related, typed, search, engine, instance, search, red, large, wheeled, vehicle, intention, finding, pictures, red, truck, search, engine, find, information, desired, matching, words, wheeled, car\".[37]\n"
     ]
    }
   ],
   "source": [
    "def get_words(doc):\n",
    "    '''Gets the word tokens for a textacy document, excluding stopwords and punc'''\n",
    "    if type(doc) == str:\n",
    "        doc = to_textacy_doc(doc)\n",
    "    return list(textacy.extract.words(doc))\n",
    "\n",
    "#print(get_words(input_doc))\n",
    "\n",
    "def get_content_words(doc):\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    return list(textacy.extract.words(doc, filter_stops=True, filter_punct=True, filter_nums=False, include_pos=None, exclude_pos=None, min_freq=1))\n",
    "print(get_content_words(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:49:27.733469Z",
     "start_time": "2017-09-19T00:49:27.705594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'Jane': {'label': 'PERSON', 'text': 'Jane'},\n",
       "             'Paul': {'label': 'PERSON', 'text': 'Paul'}})"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_named_entities(doc):\n",
    "    nes = textacy.extract.named_entities(doc)\n",
    "    return [ne for ne in nes]\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    '''Given a text document, extracts named entities using spaCy and builds a dict of metadata for each.\n",
    "    \n",
    "    Example Entity Type Labels:\n",
    "    ORGANIZATION\tGeorgia-Pacific Corp., WHO\n",
    "    PERSON\tEddy Bonte, President Obama\n",
    "    LOCATION\tMurray River, Mount Everest\n",
    "    DATE\tJune, 2008-06-29\n",
    "    TIME\ttwo fifty a m, 1:30 p.m.\n",
    "    MONEY\t175 million Canadian Dollars, GBP 10.40\n",
    "    PERCENT\ttwenty pct, 18.75 %\n",
    "    FACILITY\tWashington Monument, Stonehenge\n",
    "    GPE\tSouth East Asia, Midlothian\n",
    "    '''\n",
    "    \n",
    "    doc = to_spacy_doc(text)\n",
    "    named_entities = defaultdict(dict)\n",
    "    for ent in doc.ents:\n",
    "        ent_name = ent.text\n",
    "        named_entities[ent_name]['label'] = ent.label_\n",
    "        named_entities[ent_name]['text'] = ent.text\n",
    "        wiki_url = None #get_wiki_page(str(ent))['url']\n",
    "        if wiki_url:\n",
    "            named_entities[ent_name]['url'] = wiki_url\n",
    "        \n",
    "    return named_entities\n",
    "extract_named_entities(\"Paul is a man. Jane is a woman.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Readability Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:55.494332Z",
     "start_time": "2017-09-19T00:05:54.854211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'automated_readability_index': 22.352141689526015,\n",
       " 'coleman_liau_index': 16.682927688311693,\n",
       " 'flesch_kincaid_grade_level': 19.052077766395673,\n",
       " 'flesch_readability_ease': 19.26507900874003,\n",
       " 'gulpease_index': 41.068861371186955,\n",
       " 'gunning_fog_index': 22.472696136278014,\n",
       " 'lix': 72.24939361765064,\n",
       " 'smog_index': 18.80095838887095,\n",
       " 'wiener_sachtextformel': 12.265282847241465}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_readability_stats(doc):\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    ts = textacy.text_stats.TextStats(doc)\n",
    "    return ts.readability_stats\n",
    "\n",
    "get_readability_stats(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:55.592012Z",
     "start_time": "2017-09-19T00:05:55.497166Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-23d06b751622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msentences_readability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "def sentences_readability(sentences):\n",
    "    try:\n",
    "        sentence_stats = [get_readability_stats(str(sent)) for sent in sentences]\n",
    "        return sentence_stats\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "#sentences_readability(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:06:27.395980Z",
     "start_time": "2017-09-19T00:06:27.090961Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sentences(doc):\n",
    "    '''Returns a list of spacy spans.'''\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    return list(doc.sents)\n",
    "\n",
    "sentences = get_sentences(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Key Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get key terms from semantic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:53:21.709244Z",
     "start_time": "2017-09-19T00:53:21.517514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dvd', 'motor', 'research', 'autism', 'home', 'phase']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_semantic_key_terms(doc, top_n_terms=10, filtered=True):\n",
    "    '''Gets key terms from semantic network. '''\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    term_prob_pairs = textacy.keyterms.key_terms_from_semantic_network(doc)\n",
    "    max_keyterm_weight = term_prob_pairs[0][1]\n",
    "    \n",
    "    # keep keyterms if they're at least half as important as the most important keyterm\n",
    "    # term[0] is the word, term[1] is its keyterm-ness.\n",
    "    if filtered:\n",
    "        terms = [[term[0], term[1]] for term in term_prob_pairs if term[1] >= 0.5*(max_keyterm_weight)]\n",
    "    else:\n",
    "        terms = term_prob_pairs\n",
    "    \n",
    "    #textacy.keyterms.aggregate_term_variants(terms) #aggregates terms that are variations of each other\n",
    "    \n",
    "    return [term[0] for term in terms[:top_n_terms]]\n",
    "\n",
    "get_semantic_key_terms(to_textacy_doc(raw_abstracts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Extract Keyterms with SGRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:06:27.494564Z",
     "start_time": "2017-09-19T00:06:27.448409Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'is_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-32c6db8f1e02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyterms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/textacy/keyterms.py\u001b[0m in \u001b[0;36msgrank\u001b[0;34m(doc, ngrams, normalize, window_width, n_keyterms, idf)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# paired with positional index in document and length in a 3-tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lemma'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lower'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/textacy/keyterms.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# paired with positional index in document and length in a 3-tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lemma'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lower'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/textacy/extract.py\u001b[0m in \u001b[0;36mngrams\u001b[0;34m(doc, n, filter_stops, filter_punct, filter_nums, include_pos, exclude_pos, min_freq)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmin_freq\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mngrams_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mfreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertoolz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngrams_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         ngrams_ = (ngram for ngram in ngrams_\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/textacy/extract.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0minclude_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minclude_pos\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             ngrams_ = (ngram for ngram in ngrams_\n\u001b[0m\u001b[1;32m    157\u001b[0m                        if all(w.pos_ in include_pos for w in ngram))\n\u001b[1;32m    158\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/textacy/extract.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    if not ngram[0].is_stop and not ngram[-1].is_stop)\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilter_punct\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         ngrams_ = (ngram for ngram in ngrams_\n\u001b[0m\u001b[1;32m    145\u001b[0m                    if not any(w.is_punct for w in ngram))\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilter_nums\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/textacy/extract.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m                if not any(w.is_space for w in ngram))\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilter_stops\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         ngrams_ = (ngram for ngram in ngrams_\n\u001b[0m\u001b[1;32m    142\u001b[0m                    if not ngram[0].is_stop and not ngram[-1].is_stop)\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilter_punct\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/textacy/extract.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    137\u001b[0m                for i in range(len(doc) - n + 1))\n\u001b[1;32m    138\u001b[0m     ngrams_ = (ngram for ngram in ngrams_\n\u001b[0;32m--> 139\u001b[0;31m                if not any(w.is_space for w in ngram))\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilter_stops\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         ngrams_ = (ngram for ngram in ngrams_\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/textacy/extract.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    137\u001b[0m                for i in range(len(doc) - n + 1))\n\u001b[1;32m    138\u001b[0m     ngrams_ = (ngram for ngram in ngrams_\n\u001b[0;32m--> 139\u001b[0;31m                if not any(w.is_space for w in ngram))\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilter_stops\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         ngrams_ = (ngram for ngram in ngrams_\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'is_space'"
     ]
    }
   ],
   "source": [
    "#textacy.keyterms.sgrank(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Keyterms with TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.392763Z",
     "start_time": "2017-09-19T00:07:08.389692Z"
    }
   },
   "outputs": [],
   "source": [
    "#textacy.keyterms.textrank(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Extract acronyms and their definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.403657Z",
     "start_time": "2017-09-19T00:07:08.399477Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc = abstracts[0]\n",
    "sample_abstract = raw_abstracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.433716Z",
     "start_time": "2017-09-19T00:07:08.413423Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DVD': 'Developmental Verbal Dyspraxia'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textacy.extract.acronyms_and_definitions(to_textacy_doc(sample_abstract))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "### Extract semantic chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.450531Z",
     "start_time": "2017-09-19T00:07:08.437565Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc(129 tokens; \"This project involves discovering how the Ameri...\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(project,\n",
       "  involves,\n",
       "  discovering how the American Revolution was remembered during the nineteenth century.  )]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Semi-structured statements\n",
    "doc = abstracts[1]\n",
    "print(doc)\n",
    "list(textacy.extract.semistructured_statements(doc, \"project\", cue=u'involve'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.465877Z",
     "start_time": "2017-09-19T00:07:08.457555Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(project, involves, discovering),\n",
       " (goal, is, to show),\n",
       " (John Marshall, attempted, to use),\n",
       " (creator, attempted, to use),\n",
       " (John Calhoun, attempted, to use),\n",
       " (research, highlights, importance)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Extract Subject-Verb-Object Triples\n",
    "list(textacy.extract.subject_verb_object_triples(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.477963Z",
     "start_time": "2017-09-19T00:07:08.468414Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_pos_tagged_sents_from_corpus(textacy_corpus):\n",
    "    '''Returns a list of documents, each composed of list of sentences.\n",
    "    Sentences are lists of tuples of the form (token, POS)'''\n",
    "    return [doc.pos_tagged_text for doc in textacy_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.516016Z",
     "start_time": "2017-09-19T00:07:08.480675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['involves', 'discovering', 'was', 'remembered', 'is', 'show', 'was', 'memorialized', 'has', 'been', 'done', 'examining', 'becomes', 'attempted', 'use', 'bolster', 'citing', 'Founding', 'showing', 'lives', 'highlights', 'shaping']\n",
      "{'-PRON-': 2,\n",
      " '1800': 1,\n",
      " 'action': 2,\n",
      " 'american': 4,\n",
      " 'attempt': 1,\n",
      " 'bolster': 1,\n",
      " 'calhoun': 1,\n",
      " 'case': 1,\n",
      " 'century': 1,\n",
      " 'cite': 1,\n",
      " 'claim': 1,\n",
      " 'clear': 1,\n",
      " 'court': 1,\n",
      " 'creator': 1,\n",
      " 'crisis': 1,\n",
      " 'discover': 1,\n",
      " 'doctrine': 1,\n",
      " 'event': 2,\n",
      " 'examination': 1,\n",
      " 'examine': 1,\n",
      " 'father': 1,\n",
      " 'found': 1,\n",
      " 'goal': 1,\n",
      " 'government': 2,\n",
      " 'highlight': 1,\n",
      " 'importance': 1,\n",
      " 'involve': 1,\n",
      " 'john': 3,\n",
      " 'live': 1,\n",
      " 'marshall': 2,\n",
      " 'memorialize': 1,\n",
      " 'memory': 1,\n",
      " 'nineteenth': 1,\n",
      " 'nullification': 2,\n",
      " 'project': 1,\n",
      " 'remember': 1,\n",
      " 'research': 1,\n",
      " 'revolution': 5,\n",
      " 'shape': 1,\n",
      " 'speech': 1,\n",
      " 'states': 2,\n",
      " 'supreme': 1,\n",
      " 'united': 2,\n",
      " 'use': 1}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "def extract_verbs(doc):\n",
    "    '''Returns a list of strings that are verb-tagged tokens.'''\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    all_token_pos_pairs = itertools.chain(*doc.pos_tagged_text) #flatten list\n",
    "    verbs = [token for token, pos in all_token_pos_pairs if pos.startswith(\"V\")]\n",
    "    return verbs\n",
    "print(extract_verbs(doc))\n",
    "\n",
    "def bag_of_words(doc, as_strings=True):\n",
    "    '''Returns a dictionary with word:count pairs. Words are grouped by lemma.'''\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    return doc.to_bag_of_words(as_strings=as_strings)\n",
    "      \n",
    "pprint(bag_of_words(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:58:27.722628Z",
     "start_time": "2017-09-12T19:58:27.712079Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Wikipedia wrapper (API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.646294Z",
     "start_time": "2017-09-19T00:07:08.526459Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from wikipedia import DisambiguationError, PageError, RedirectError\n",
    "\n",
    "def get_wiki_page(search_string, summary=False, content=False):\n",
    "    '''Returns results from searching for search_string with wikipedia wrapper library. \n",
    "       Note: Makes a web request'''\n",
    "    try:\n",
    "        page = wikipedia.page(search_string)\n",
    "        page_data = {\"url\":page.url,\n",
    "                     \"title\":page.title}\n",
    "        if content:\n",
    "            page_data[\"content\"] = page.content # Full text content of page.\n",
    "        if summary:\n",
    "            page_data[\"summary\"] = page.summary # Summary section only.\n",
    "    \n",
    "    except DisambiguationError as e:\n",
    "        return get_wiki_page(e.options[0]) #naively choose first option\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "    return page_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.655811Z",
     "start_time": "2017-09-19T00:07:08.648557Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_doc = \"London\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.707893Z",
     "start_time": "2017-09-19T00:07:08.659514Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'html5lib.sanitizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-1541144f2948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbleach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mset_link_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'title'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mu'AI-provided Link'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/bleach/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhtml5lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhtml5lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msanitizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTMLSanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhtml5lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtmlserializer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTMLSerializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'html5lib.sanitizer'"
     ]
    }
   ],
   "source": [
    "import bleach\n",
    "\n",
    "def set_link_title(attrs, new=False):\n",
    "    attrs[(None, u'title')] = u'AI-provided Link'\n",
    "    return attrs\n",
    "\n",
    "def linkify(string):\n",
    "    '''Calls bleach.linkify.\n",
    "    Converts urls in the input string into links. \n",
    "    Returns a string of HTML.'''\n",
    "    if type(string) != str:\n",
    "        raise TypeError(\"input should be a string\")\n",
    "        \n",
    "    linker = Linker(callbacks=[set_link_title])\n",
    "    return bleach.linkify(string)\n",
    "\n",
    "def create_hyperlink(url, display_text, attrs=\"\"):\n",
    "    '''Optional attrs is a string of tag attributes.\n",
    "       Example call:\n",
    "       create_hyperlink('www.google.com', 'Google', \n",
    "       ... attrs = 'class=link_class title=\"Custom Title\"')'''\n",
    "    hyperlink_format = '<a href=\"{link}\" {attrs}>{text}</a>'\n",
    "    return hyperlink_format.format(link=url, attrs=attrs, text=display_text)\n",
    "\n",
    "create_hyperlink('www.google.com', 'Google', attrs='class=link_class title=\"Custom Title\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.709016Z",
     "start_time": "2017-09-19T00:07:08.493Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### HTML-related functions\n",
    "def create_hyperlink(url, display_text, attrs=\"\"):\n",
    "    '''Optional attrs is a string of tag attributes.\n",
    "       \n",
    "       create_hyperlink('www.google.com', 'Google', \n",
    "       ... attrs = 'class=link_class title=\"Custom Title\"')'''\n",
    "    \n",
    "    hyperlink_format = '<a href=\"{link}\" {attrs}>{text}</a>'\n",
    "    return hyperlink_format.format(link=url, attrs=attrs, text=display_text)\n",
    "\n",
    "def linkify_entity(ent_dict):\n",
    "    '''Operates on extracted named entities. Returns HTML string.'''\n",
    "    ent_type = ent_dict['label']\n",
    "    text = ent_dict['text'] \n",
    "    url = get_wiki_page(text)['url']\n",
    "    attrs = 'class=\"{ent_type}\" title=\"{text}\"'.format(ent_type=ent_type, text=text)\n",
    "    return create_hyperlink(url, text, attrs=attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.710515Z",
     "start_time": "2017-09-19T00:07:08.501Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_doc = \"London is a city in the U.K.\"\n",
    "london = extract_named_entities(test_doc)[\"London\"]\n",
    "print(london)\n",
    "print(linkify_entity(london))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Wiktionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.711473Z",
     "start_time": "2017-09-19T00:07:08.513Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from wiktionaryparser import WiktionaryParser, WikiParse\n",
    "parser = WiktionaryParser()\n",
    "word = parser.fetch('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Wikipedia XML Dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T06:45:16.997470Z",
     "start_time": "2017-09-13T06:45:16.994459Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### http://www.heatonresearch.com/2017/03/03/python-basic-wikipedia-parsing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.712190Z",
     "start_time": "2017-09-19T00:07:08.541Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as etree\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "PATH_WIKI_XML = '/Users/davideverling/Projects/Data'\n",
    "FILENAME_WIKI = 'enwiki-latest-pages-articles.xml'\n",
    "FILENAME_ARTICLES = 'articles.csv'\n",
    "FILENAME_REDIRECT = 'articles_redirect.csv'\n",
    "FILENAME_TEMPLATE = 'articles_template.csv'\n",
    "ENCODING = \"utf-8\"\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "def strip_tag_name(t):\n",
    "    t = elem.tag\n",
    "    idx = k = t.rfind(\"}\")\n",
    "    if idx != -1:\n",
    "        t = t[idx + 1:]\n",
    "    return t\n",
    "\n",
    "pathWikiXML = os.path.join(PATH_WIKI_XML, FILENAME_WIKI)\n",
    "pathArticles = os.path.join(PATH_WIKI_XML, FILENAME_ARTICLES)\n",
    "pathArticlesRedirect = os.path.join(PATH_WIKI_XML, FILENAME_REDIRECT)\n",
    "pathTemplateRedirect = os.path.join(PATH_WIKI_XML, FILENAME_TEMPLATE)\n",
    "\n",
    "totalCount = 0\n",
    "articleCount = 0\n",
    "redirectCount = 0\n",
    "templateCount = 0\n",
    "title = None\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.714575Z",
     "start_time": "2017-09-19T00:07:08.560Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''Begin streaming the XML file and \n",
    "write the headers for the 3 CSV files that will be built according to the data found in the XML.'''\n",
    "\n",
    "with codecs.open(pathArticles, \"w\", ENCODING) as articlesFH, \\\n",
    "        codecs.open(pathArticlesRedirect, \"w\", ENCODING) as redirectFH, \\\n",
    "        codecs.open(pathTemplateRedirect, \"w\", ENCODING) as templateFH:\n",
    "    articlesWriter = csv.writer(articlesFH, quoting=csv.QUOTE_MINIMAL)\n",
    "    redirectWriter = csv.writer(redirectFH, quoting=csv.QUOTE_MINIMAL)\n",
    "    templateWriter = csv.writer(templateFH, quoting=csv.QUOTE_MINIMAL)\n",
    "    articlesWriter.writerow(['id', 'title', 'redirect'])\n",
    "    redirectWriter.writerow(['id', 'title', 'redirect'])\n",
    "    templateWriter.writerow(['id', 'title'])\n",
    "\n",
    "    for event, elem in etree.iterparse(pathWikiXML, events=('start', 'end')):\n",
    "        tname = strip_tag_name(elem.tag)\n",
    "        if event == 'start':\n",
    "            if tname == 'page':\n",
    "                title = ''\n",
    "                id = -1\n",
    "                redirect = ''\n",
    "                inrevision = False\n",
    "                ns = 0\n",
    "            elif tname == 'revision':\n",
    "                # Do not pick up on revision id's\n",
    "                inrevision = True\n",
    "        else:\n",
    "            if tname == 'title':\n",
    "                title = elem.text\n",
    "            elif tname == 'id' and not inrevision:\n",
    "                id = int(elem.text)\n",
    "            elif tname == 'redirect':\n",
    "                redirect = elem.attrib['title']\n",
    "            elif tname == 'ns':\n",
    "                ns = int(elem.text)\n",
    "            elif tname == 'page':\n",
    "                totalCount += 1\n",
    "                if ns == 10:\n",
    "                    templateCount += 1\n",
    "                    templateWriter.writerow([id, title])\n",
    "                elif len(redirect) > 0:\n",
    "                    articleCount += 1\n",
    "                    articlesWriter.writerow([id, title, redirect])\n",
    "                else:\n",
    "                    redirectCount += 1\n",
    "                    redirectWriter.writerow([id, title, redirect])\n",
    "\n",
    "        if totalCount > 1 and (totalCount % 100000) == 0:\n",
    "            print(\"{:,}\".format(totalCount))\n",
    "\n",
    "    elem.clear()\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Total pages: {:,}\".format(totalCount))\n",
    "    print(\"Template pages: {:,}\".format(templateCount))\n",
    "    print(\"Article pages: {:,}\".format(articleCount))\n",
    "    print(\"Redirect pages: {:,}\".format(redirectCount))\n",
    "    print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dbpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T02:30:08.138514Z",
     "start_time": "2017-09-19T02:30:08.081843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<ArrayOfResult xmlns=\"http://lookup.dbpedia.org/\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\\n      <Result>\\n      <Label>Drum</Label>\\n      <URI>http://dbpedia.org/resource/Drum</URI>\\n      <Description>The drum is a member of the percussion group of musical instruments. In the Hornbostel-Sachs classification system, it is a membranophone. Drums consist of at least one membrane, called a drumhead or drum skin, that is stretched over a shell and struck, either directly with the player\\'s hands, or with a drum stick, to produce sound. There is usually a resonance head on the underside of the drum, typically tuned to a slightly lower pitch than the top drumhead.</Description>\\n      <Classes><Class><Label>owl#Thing</Label><URI>http://www.w3.org/2002/07/owl#Thing</URI></Class></Classes>\\n      <Categories><Category><Label>Drums</Label><URI>http://dbpedia.org/resource/Category:Drums</URI></Category><Category><Label>Membranophones</Label><URI>http://dbpedia.org/resource/Category:Membranophones</URI></Category></Categories>\\n      <Templates></Templates>\\n      <Redirects></Redirects>\\n      <Refcount>3577</Refcount>\\n    </Result><Result>\\n      <Label>Banana</Label>\\n      <URI>http://dbpedia.org/resource/Banana</URI>\\n      <Description>The banana is an edible fruit, botanically a berry, produced by several kinds of large herbaceous flowering plants in the genus Musa. In some countries, bananas used for cooking may be called plantains. The fruit is variable in size, color and firmness, but is usually elongated and curved, with soft flesh rich in starch covered with a rind which may be green, yellow, red, purple, or brown when ripe. The fruits grow in clusters hanging from the top of the plant.</Description>\\n      <Classes><Class><Label>owl#Thing</Label><URI>http://www.w3.org/2002/07/owl#Thing</URI></Class></Classes>\\n      <Categories><Category><Label>Staple foods</Label><URI>http://dbpedia.org/resource/Category:Staple_foods</URI></Category><Category><Label>Cormous plants</Label><URI>http://dbpedia.org/resource/Category:Cormous_plants</URI></Category><Category><Label>Edible fruits</Label><URI>http://dbpedia.org/resource/Category:Edible_fruits</URI></Category><Category><Label>Tropical fruit</Label><URI>http://dbpedia.org/resource/Category:Tropical_fruit</URI></Category><Category><Label>Tropical agriculture</Label><URI>http://dbpedia.org/resource/Category:Tropical_agriculture</URI></Category><Category><Label>Allergenic foods</Label><URI>http://dbpedia.org/resource/Category:Allergenic_foods</URI></Category><Category><Label>Fiber plants</Label><URI>http://dbpedia.org/resource/Category:Fiber_plants</URI></Category><Category><Label>Bananas</Label><URI>http://dbpedia.org/resource/Category:Bananas</URI></Category></Categories>\\n      <Templates></Templates>\\n      <Redirects></Redirects>\\n      <Refcount>2574</Refcount>\\n    </Result><Result>\\n      <Label>Cooking plantain</Label>\\n      <URI>http://dbpedia.org/resource/Cooking_plantain</URI>\\n      <Description>A plantain, or cooking plantain, (/\\xcb\\x88pl\\xc3\\xa6nt\\xc9\\xa8n/, US /pl\\xc3\\xa6n\\xcb\\x88te\\xc9\\xaan/, UK /\\xcb\\x88pl\\xc9\\x91\\xcb\\x90nt\\xc9\\xa8n/) is one of the less sweet cultivated varieties (cultivars) of the genus Musa whose fruit is also known as the banana.</Description>\\n      <Classes><Class><Label>http://www.wikidata.org/entity/ q19088</Label><URI>http://www.wikidata.org/entity/Q19088</URI></Class><Class><Label>http://www.wikidata.org/entity/ q756</Label><URI>http://www.wikidata.org/entity/Q756</URI></Class><Class><Label>plant</Label><URI>http://dbpedia.org/ontology/Plant</URI></Class><Class><Label>http://www.wikidata.org/entity/ q4886</Label><URI>http://www.wikidata.org/entity/Q4886</URI></Class><Class><Label>owl#Thing</Label><URI>http://www.w3.org/2002/07/owl#Thing</URI></Class><Class><Label>species</Label><URI>http://dbpedia.org/ontology/Species</URI></Class><Class><Label>eukaryote</Label><URI>http://dbpedia.org/ontology/Eukaryote</URI></Class><Class><Label>cultivated variety</Label><URI>http://dbpedia.org/ontology/CultivatedVariety</URI></Class></Classes>\\n      <Categories><Category><Label>Puerto Rican cuisine</Label><URI>http://dbpedia.org/resource/Category:Puerto_Rican_cuisine</URI></Category><Category><Label>Plantain dishes</Label><URI>http://dbpedia.org/resource/Category:Plantain_dishes</URI></Category><Category><Label>Nigerian cuisine</Label><URI>http://dbpedia.org/resource/Category:Nigerian_cuisine</URI></Category><Category><Label>Staple foods</Label><URI>http://dbpedia.org/resource/Category:Staple_foods</URI></Category><Category><Label>Yoruba cuisine</Label><URI>http://dbpedia.org/resource/Category:Yoruba_cuisine</URI></Category><Category><Label>Cuban cuisine</Label><URI>http://dbpedia.org/resource/Category:Cuban_cuisine</URI></Category><Category><Label>Jamaican cuisine</Label><URI>http://dbpedia.org/resource/Category:Jamaican_cuisine</URI></Category><Category><Label>Haitian cuisine</Label><URI>http://dbpedia.org/resource/Category:Haitian_cuisine</URI></Category><Category><Label>Honduran cuisine</Label><URI>http://dbpedia.org/resource/Category:Honduran_cuisine</URI></Category><Category><Label>Tropical fruit</Label><URI>http://dbpedia.org/resource/Category:Tropical_fruit</URI></Category><Category><Label>Tropical agriculture</Label><URI>http://dbpedia.org/resource/Category:Tropical_agriculture</URI></Category><Category><Label>Fruit vegetables</Label><URI>http://dbpedia.org/resource/Category:Fruit_vegetables</URI></Category><Category><Label>Bananas</Label><URI>http://dbpedia.org/resource/Category:Bananas</URI></Category></Categories>\\n      <Templates></Templates>\\n      <Redirects></Redirects>\\n      <Refcount>629</Refcount>\\n    </Result><Result>\\n      <Label>Double act</Label>\\n      <URI>http://dbpedia.org/resource/Double_act</URI>\\n      <Description>A double act, also known as a comedy duo, is a comic pairing in which humor is derived from the uneven relationship between two partners, usually of the same gender, age, ethnic origin and profession but drastically different in terms of personality or behavior.</Description>\\n      <Classes><Class><Label>http://www.ontologydesignpatterns.org/ont/dul/ d u l.owl# concept</Label><URI>http://www.ontologydesignpatterns.org/ont/dul/DUL.owl#Concept</URI></Class><Class><Label>topical concept</Label><URI>http://dbpedia.org/ontology/TopicalConcept</URI></Class><Class><Label>genre</Label><URI>http://dbpedia.org/ontology/Genre</URI></Class><Class><Label>owl#Thing</Label><URI>http://www.w3.org/2002/07/owl#Thing</URI></Class><Class><Label>http://www.wikidata.org/entity/ q188451</Label><URI>http://www.wikidata.org/entity/Q188451</URI></Class></Classes>\\n      <Categories><Category><Label>Comedy characters</Label><URI>http://dbpedia.org/resource/Category:Comedy_characters</URI></Category><Category><Label>Comedy duos</Label><URI>http://dbpedia.org/resource/Category:Comedy_duos</URI></Category><Category><Label>Vaudeville tropes</Label><URI>http://dbpedia.org/resource/Category:Vaudeville_tropes</URI></Category><Category><Label>Counterparts to the protagonist</Label><URI>http://dbpedia.org/resource/Category:Counterparts_to_the_protagonist</URI></Category></Categories>\\n      <Templates></Templates>\\n      <Redirects></Redirects>\\n      <Refcount>466</Refcount>\\n    </Result><Result>\\n      <Label>Bonanno crime family</Label>\\n      <URI>http://dbpedia.org/resource/Bonanno_crime_family</URI>\\n      <Description>The Bonanno crime family is one of the &quot;Five Families&quot; that dominates organized crime activities in New York City, United States, within the nationwide criminal phenomenon known as the American Mafia (or Cosa Nostra).Founded by and named after Joseph Bonanno, for over 30 years the family was one of the most powerful in the country.  However, in the early 1960s, Bonanno attempted to seize the mantle of boss of bosses, but failed and was forced to retire.</Description>\\n      <Classes></Classes>\\n      <Categories><Category><Label>Gangs in New York City</Label><URI>http://dbpedia.org/resource/Category:Gangs_in_New_York_City</URI></Category><Category><Label>Italian-American crime families</Label><URI>http://dbpedia.org/resource/Category:Italian-American_crime_families</URI></Category><Category><Label>Bonanno crime family</Label><URI>http://dbpedia.org/resource/Category:Bonanno_crime_family</URI></Category><Category><Label>Five Families</Label><URI>http://dbpedia.org/resource/Category:Five_Families</URI></Category></Categories>\\n      <Templates></Templates>\\n      <Redirects></Redirects>\\n      <Refcount>352</Refcount>\\n    </Result>\\n    </ArrayOfResult>'\n",
      "{'label': 'Drum', 'uri': 'http://dbpedia.org/resource/Drum', 'description': \"The drum is a member of the percussion group of musical instruments. In the Hornbostel-Sachs classification system, it is a membranophone. Drums consist of at least one membrane, called a drumhead or drum skin, that is stretched over a shell and struck, either directly with the player's hands, or with a drum stick, to produce sound. There is usually a resonance head on the underside of the drum, typically tuned to a slightly lower pitch than the top drumhead.\"}\n",
      "{'label': 'Machine learning', 'uri': 'http://dbpedia.org/resource/Machine_learning', 'description': 'Machine learning is a subfield of computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from xml.etree import ElementTree\n",
    "from collections import defaultdict\n",
    "\n",
    "def dbpedia_keyword_search(keywords, api_host='http://localhost:1111', query_class=''):\n",
    "    api_string = api_host+\"/api/search/KeywordSearch?\"\n",
    "    query_class = 'QueryClass=' + query_class + \"&\"\n",
    "    query_string = 'QueryString=' + keywords\n",
    "    request_string = api_string + query_class + query_string\n",
    "    \n",
    "    response = requests.get(request_string)  \n",
    "    xml_tree = ElementTree.fromstring(response.content)\n",
    "    return response.content\n",
    "\n",
    "def dbpedia_prefix_search(query_string, api_host='http://localhost:1111', query_class=''):\n",
    "    '''Returns list of dicts from dbpedia API search. Keys are: label, uri, description'''\n",
    "    api_string = api_host+\"/api/search/PrefixSearch?\"\n",
    "    query_class = 'QueryClass=' + query_class + \"&\"\n",
    "    query_string = 'QueryString=' + query_string\n",
    "    request_string = api_string + query_class + query_string\n",
    "    \n",
    "    response = requests.get(request_string)\n",
    "    xmltree = ElementTree.fromstring(response.content)\n",
    "    \n",
    "    lookup = './/{http://lookup.dbpedia.org/}'\n",
    "    results = xmltree.findall(lookup+\"Result\")\n",
    "    if not results:\n",
    "        return None\n",
    "    \n",
    "    results_list = []\n",
    "    for i, result in enumerate(results):\n",
    "        results_list.append({\n",
    "            'label': xmltree.findall(lookup+\"Label\")[i].text,\n",
    "            'uri'  : xmltree.findall(lookup+\"URI\")[i].text,\n",
    "            'description' : xmltree.findall(lookup+\"Description\")[i].text\n",
    "        })\n",
    "    return results_list\n",
    "\n",
    "print(dbpedia_keyword_search(\"banana\"))\n",
    "print(dbpedia_prefix_search(\"banana\")[0])\n",
    "print(dbpedia_prefix_search(\"machine lea\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:45.441434Z",
     "start_time": "2017-09-19T00:07:25.750924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Species: In biology, a species (abbreviated sp., with the plural form '\n",
      " 'species abbreviated spp.) is one of the basic units of biological '\n",
      " 'classification and a taxonomic rank. A species is often defined as the '\n",
      " 'largest group of organisms  where two hybrids are capable of reproducing '\n",
      " 'fertile offspring, typically using sexual reproduction. While in many cases '\n",
      " 'this definition is adequate, the difficulty of defining species is known as '\n",
      " 'the species problem.',\n",
      " 'Human body: The human body includes the entire structure of a human being '\n",
      " 'and comprises a head, neck, trunk (which includes the thorax and abdomen), '\n",
      " 'arms and hands, legs and feet.Every part of the body is composed of various '\n",
      " 'types of cells, the fundamental unit of life.At maturity, the estimated '\n",
      " 'average number of cells in the body is given as 37.2 trillion.  This number '\n",
      " 'is stated to be of partial data and to be used as a starting point for '\n",
      " 'further calculations.',\n",
      " 'Lexicon: A lexicon is the vocabulary of a person, language, or branch of '\n",
      " 'knowledge (such as nautical or medical).  In linguistics, a lexicon is a '\n",
      " \"language's inventory of lexemes.\",\n",
      " 'Genus: In biology, a genus /ˈdʒiːnəs/ (plural: genera) is a taxonomic rank '\n",
      " 'used in the biological classification of living and fossil organisms.   In '\n",
      " 'the hierarchy of biological classification, genus comes above species and '\n",
      " 'below family. In binomial nomenclature, the genus name forms the first part '\n",
      " 'of the binomial species name for each species within the genus.E.G., Pongo '\n",
      " 'pygmaeus and Pongo abelii are two species within the genus Pongo.',\n",
      " 'India: India, officially the Republic of India (Bhārat Gaṇarājya), is a '\n",
      " 'country in South Asia. It is the seventh-largest country by area, the '\n",
      " 'second-most populous country with over 1.2 billion people, and the most '\n",
      " 'populous democracy in the world. India is a federal constitutional republic '\n",
      " 'governed under a parliamentary system consisting of 29 states and 7 union '\n",
      " 'territories.',\n",
      " 'Artificial intelligence: Artificial intelligence (AI) is the intelligence '\n",
      " 'exhibited by machines or software. It is also the name of the academic field '\n",
      " 'of study which studies how to create computers and computer software that '\n",
      " 'are capable of intelligent behavior. Major AI researchers and textbooks '\n",
      " 'define this field as \"the study and design of intelligent agents\", in which '\n",
      " 'an intelligent agent is a system that perceives its environment and takes '\n",
      " 'actions that maximize its chances of success.',\n",
      " 'Thor (Marvel Comics): Thor is a fictional superhero that appears in American '\n",
      " 'comic books published by Marvel Comics. The character, based on the Norse '\n",
      " 'mythological deity of the same name, is the Asgardian god of thunder and '\n",
      " 'possesses the enchanted hammer Mjolnir, which grants him the ability of '\n",
      " 'flight and weather manipulation amongst his other superhuman '\n",
      " 'attributes.Debuting in the Silver Age of Comic Books, the character first '\n",
      " 'appeared in Journey into Mystery #83 (Aug.',\n",
      " 'Bird: Birds (class Aves) are a group of endothermic vertebrates, '\n",
      " 'characterised by feathers, a beak with no teeth, the laying of hard-shelled '\n",
      " 'eggs, a high metabolic rate, a four-chambered heart, and a lightweight but '\n",
      " 'strong skeleton. Birds live worldwide and range in size from the 5 cm (2 in) '\n",
      " 'bee hummingbird to the 2.75 m (9 ft) ostrich.',\n",
      " 'Character (arts): A character (or fictional character) is a person in a '\n",
      " 'narrative work of art (such as a novel, play, television series or film). '\n",
      " 'Derived from the ancient Greek word χαρακτήρ, the English word dates from '\n",
      " 'the Restoration, although it became widely used after its appearance in Tom '\n",
      " 'Jones  in 1749. From this, the sense of \"a part played by an actor\" '\n",
      " 'developed.',\n",
      " 'Appeal: In law, an appeal is the process in which cases are reviewed, where '\n",
      " 'parties request a formal change to an official decision. Appeals function '\n",
      " 'both as a process for error correction as well as a process of clarifying '\n",
      " 'and interpreting law. Although appellate courts have existed for thousands '\n",
      " 'of years, common law countries did not incorporate an affirmative right to '\n",
      " 'appeal into their jurisprudence until the nineteenth century.',\n",
      " 'Schizophrenia: Schizophrenia (/ˌskɪtsɵˈfrɛniə/ or /ˌskɪtsɵˈfriːniə/) is a '\n",
      " 'mental disorder often characterized by abnormal social behavior and failure '\n",
      " 'to recognize what is real. Common symptoms include false beliefs, unclear or '\n",
      " 'confused thinking, auditory hallucinations, reduced social engagement and '\n",
      " 'emotional expression, and lack of motivation.',\n",
      " 'Autopsy: An autopsy—also known as a post-mortem examination, necropsy, '\n",
      " 'autopsia cadaverum, or obduction—is a highly specialized surgical procedure '\n",
      " 'that consists of a thorough examination of a corpse to determine the cause '\n",
      " 'and manner of death and to evaluate any disease or injury that may be '\n",
      " 'present. It is usually performed by a specialized medical doctor called a '\n",
      " 'pathologist.The word “autopsy” means to study and directly observe the body '\n",
      " '(Adkins and Barnes, 317).',\n",
      " 'IllScarlett: IllScarlett (styled as illScarlett; /ˌɪlˈskɑrlɨt/) is a '\n",
      " 'Canadian rock and reggae band.Their sound can be defined as pop infused rock '\n",
      " 'reggae. They have made a name for themselves in the Canadian music scene and '\n",
      " 'are making their name known throughout the United States and around the '\n",
      " 'world. Their most prominent influence is California-based band Sublime.The '\n",
      " 'band found their break when they decided to play outside the 2004 Vans '\n",
      " 'Warped Tour venue in front of fans waiting in line to get in.',\n",
      " 'Type (biology): In biology, a type is one particular specimen (or in some '\n",
      " 'cases a group of specimens) of an organism to which the scientific name of '\n",
      " 'that organism is formally attached. In other words, a type is an example '\n",
      " 'that serves to anchor or centralize the defining features of that particular '\n",
      " 'taxon.',\n",
      " 'Marks & Spencer: Marks and Spencer plc (also known as M&S) is a major '\n",
      " 'British multinational retailer headquartered in the City of Westminster, '\n",
      " 'London. It specialises in the selling of clothing, home products and luxury '\n",
      " 'food products.',\n",
      " 'Semantics: Semantics (from Ancient Greek: σημαντικός sēmantikós, '\n",
      " '\"significant\") is the study of meaning. It focuses on the relation between '\n",
      " 'signifiers, like words, phrases, signs, and symbols, and what they stand '\n",
      " 'for; their denotation. Linguistic semantics is the study of meaning that is '\n",
      " 'used for understanding human expression through language.',\n",
      " 'Sampler (musical instrument): A sampler (or, in the case of only playback, '\n",
      " 'called sample player or Rompler), is an electronic musical instrument '\n",
      " 'similar in some respects to a synthesizer but, instead of generating sounds, '\n",
      " 'it uses recordings (or \"samples\") of sounds that are loaded or recorded into '\n",
      " 'it by the user, and then played back by means of the sampler program itself, '\n",
      " 'a keyboard, sequencer or other triggering device, to perform or compose '\n",
      " 'music.',\n",
      " 'Bill Gaither (gospel singer): William J. \"Bill\" Gaither (born March 28, '\n",
      " '1936) is an American singer and songwriter of southern gospel and '\n",
      " 'Contemporary Christian music. Besides having written numerous popular '\n",
      " 'Christian songs with his wife, Gloria, he is also known for performing as '\n",
      " 'part of the Bill Gaither Trio, and the Gaither Vocal Band. In the 1990s, his '\n",
      " 'career (as well as the careers of other southern gospel artists) gained a '\n",
      " 'resurgence as the popularity of the Gaither Homecoming series grew.',\n",
      " 'Phrase: In everyday speech, a phrase may be any group of words, often '\n",
      " 'carrying a special idiomatic meaning; in this sense it is roughly synonymous '\n",
      " 'with expression. In linguistic analysis, a phrase is a group of words (or '\n",
      " 'possibly a single word) that functions as a constituent in the syntax of a '\n",
      " 'sentence—a single unit within a grammatical hierarchy. A phrase appears '\n",
      " 'within a clause, although it is also possible for a phrase to be a clause or '\n",
      " 'to contain a clause within it.',\n",
      " 'Chemical reaction: A chemical reaction is a process that leads to the '\n",
      " 'transformation of one set of chemical substances to another. Classically, '\n",
      " 'chemical reactions encompass changes that only involve the positions of '\n",
      " 'electrons in the forming and breaking of chemical bonds between atoms, with '\n",
      " 'no change to the nuclei (no change to the elements present), and can often '\n",
      " 'be described by a chemical equation.',\n",
      " 'Moreover Technologies: Moreover Technologies (generally known as \"Moreover\") '\n",
      " 'is a provider of business intelligence and news aggregation products for '\n",
      " 'enterprises, also offering free news feeds for consumers. Moreover was '\n",
      " 'founded in 1998 by Nick Denton, David Galbraith, and Angus Bankes, receiving '\n",
      " 'funding from Advance Publications, Atlas Venture, Reuters Venture Capital '\n",
      " 'and Dawntreader Ventures, with offices in both London and San Francisco.',\n",
      " 'Glossary of professional wrestling terms: Professional wrestling has accrued '\n",
      " 'a considerable nomenclature through its long existence. Much of it stems '\n",
      " \"from the industry's origins in the days of carnivals and circuses, and the \"\n",
      " 'slang itself is often referred to as \"carny talk\". In the past, wrestlers '\n",
      " 'used such terms in the presence of fans so as not to reveal the worked '\n",
      " 'nature of the business. In recent years, widespread discussion on the '\n",
      " 'Internet has popularized these terms.',\n",
      " 'Pump: A pump is a device that moves fluids (liquids or gases), or sometimes '\n",
      " 'slurries, by mechanical action. Pumps can be classified into three major '\n",
      " 'groups according to the method they use to move the fluid: direct lift, '\n",
      " 'displacement, and gravity pumps.Pumps operate by some mechanism (typically '\n",
      " 'reciprocating or rotary), and consume energy to perform mechanical work by '\n",
      " 'moving the fluid.',\n",
      " 'God: In monotheism and henotheism, God is conceived as the Supreme Being and '\n",
      " 'principal object of faith. The concept of God as described by theologians '\n",
      " 'commonly includes the attributes of omniscience (infinite knowledge), '\n",
      " 'omnipotence (unlimited power), omnipresence (present everywhere), '\n",
      " 'omnibenevolence (perfect goodness), divine simplicity, and eternal and '\n",
      " 'necessary existence.',\n",
      " 'Sitcom: A situation comedy, often shortened to the portmanteau sitcom, is a '\n",
      " 'genre of comedy that features characters sharing the same common '\n",
      " 'environment, such as a home or workplace, with often humorous dialogue. Such '\n",
      " 'programs originated in radio, but today, sitcoms are found mostly on '\n",
      " 'television as one of its dominant narrative forms.',\n",
      " 'Adolf Hitler: Adolf Hitler (20 April 1889 – 30 April 1945) was an '\n",
      " 'Austrian-born German politician who was the leader of the Nazi Party '\n",
      " '(NSDAP), Chancellor of Germany from 1933 to 1945, and Führer (\"leader\") of '\n",
      " 'Nazi Germany from 1934 to 1945. As effective dictator of Nazi Germany, '\n",
      " 'Hitler was at the centre of World War II in Europe and the Holocaust.Hitler '\n",
      " 'was a decorated veteran of World War I. He joined the precursor of the '\n",
      " \"NSDAP, the German Workers' Party, in 1919 and became leader of the NSDAP in \"\n",
      " '1921.',\n",
      " 'Old English: Old English (Ænglisc, Anglisc, Englisc) or Anglo-Saxon is the '\n",
      " 'earliest historical form of the English language, spoken in England and '\n",
      " 'southern and eastern Scotland in the early Middle Ages. It was brought to '\n",
      " 'Great Britain by Anglo-Saxon settlers probably in the mid-5th century, and '\n",
      " 'the first Old English literary works date from the mid-7th century.',\n",
      " 'Per capita income: Per capita income or average income is the measure of the '\n",
      " 'amount of money that is being earned by person in a certain area, such as a '\n",
      " 'city, region, or country, which is calculated by dividing the total income '\n",
      " 'of a the area by its total population.',\n",
      " 'Mars Reconnaissance Orbiter: Mars Reconnaissance Orbiter (MRO) is a '\n",
      " 'multipurpose spacecraft designed to conduct reconnaissance and exploration '\n",
      " 'of Mars from orbit.  The US$720 million spacecraft was built by Lockheed '\n",
      " 'Martin under the supervision of the Jet Propulsion Laboratory.  The mission '\n",
      " 'is managed by the California Institute of Technology, at the JPL, in La '\n",
      " 'Cañada Flintridge, California, for the NASA Science Mission Directorate, '\n",
      " 'Washington, D.C.',\n",
      " 'Gene: A gene is a locus (or region) of DNA that encodes a functional RNA or '\n",
      " 'protein product, and is the molecular unit of heredity. The transmission of '\n",
      " \"genes to an organism's offspring is the basis of the inheritance of \"\n",
      " 'phenotypic traits. Most biological traits are under the influence of '\n",
      " 'polygenes (many different genes) as well as the gene–environment '\n",
      " 'interactions.',\n",
      " 'Evolution: Evolution is change in the heritable traits of biological '\n",
      " 'populations over successive generations. Evolutionary processes give rise to '\n",
      " 'diversity at every level of biological organisation, including the levels of '\n",
      " 'species, individual organisms, and  molecules.All of life on earth shares a '\n",
      " 'common ancestor known as the last universal ancestor, which lived '\n",
      " 'approximately 3.5–3.8 billion years ago.',\n",
      " 'Apparent magnitude: The apparent magnitude (m) of a celestial object is a '\n",
      " 'measure of its brightness as seen by an observer on Earth, adjusted to the '\n",
      " 'value it would have in the absence of the atmosphere. The brighter an object '\n",
      " 'appears, the lower the assigned value of its magnitude (inverse relation).',\n",
      " 'Public domain: Works in the public domain are those whose intellectual '\n",
      " 'property rights have expired, have been forfeited, or are inapplicable. '\n",
      " 'Examples include the works of Shakespeare and Beethoven, most of the early '\n",
      " 'silent films, the formulae of Newtonian physics, Serpent encryption '\n",
      " 'algorithm and powered flight.',\n",
      " 'Documentary film: A documentary film is a nonfictional motion picture '\n",
      " 'intended to document some aspect of reality, primarily for the purposes of '\n",
      " 'instruction or maintaining a historical record. Such films were originally '\n",
      " 'shot on film stock—the only medium available—but now include video and '\n",
      " 'digital productions that can be either direct-to-video, made into a TV show '\n",
      " 'or released for screening in cinemas.',\n",
      " 'Fighter aircraft: A fighter aircraft is a military aircraft designed '\n",
      " 'primarily for air-to-air combat against other aircraft, as opposed to '\n",
      " 'bombers and attack aircraft, whose main mission is to attack ground targets. '\n",
      " 'The hallmarks of a fighter are its speed, maneuverability, and small size '\n",
      " 'relative to other combat aircraft.Many fighters have secondary ground-attack '\n",
      " 'capabilities, and some are designed as dual-purpose fighter-bombers; often '\n",
      " 'aircraft that do not fulfill the standard definition are called fighters.',\n",
      " 'Shamanism: Shamanism (/ˈʃɑːmən/ SHAH-mən or /ˈʃeɪmən/ SHAY-mən) is a '\n",
      " 'practice that involves a practitioner reaching altered states of '\n",
      " 'consciousness in order to perceive and interact with a spirit world and '\n",
      " 'channel these transcendental energies into this world.',\n",
      " 'Mind: A mind /ˈmaɪnd/ is the set of cognitive faculties that enables '\n",
      " 'consciousness, perception, thinking, judgement, and memory—a characteristic '\n",
      " 'of humans, but which also may apply to other life forms.A lengthy tradition '\n",
      " 'of inquiries in philosophy, religion, psychology and cognitive science has '\n",
      " 'sought to develop an understanding of what a mind is and what its '\n",
      " 'distinguishing properties are.',\n",
      " 'Dictionary of National Biography: The Dictionary of National Biography (DNB) '\n",
      " 'is a standard work of reference on notable figures from British history, '\n",
      " 'published from 1885. The updated Oxford Dictionary of National Biography '\n",
      " '(ODNB) was published on 23 September 2004 in 60 volumes and online.',\n",
      " 'Michael Jackson: Michael Joseph Jackson (August 29, 1958 – June 25, 2009) '\n",
      " 'was an American singer, songwriter, record producer, dancer, and actor.',\n",
      " 'Soviet Union: The Union of Soviet Socialist Republics (Russian: Сою́з '\n",
      " 'Сове́тских Социалисти́ческих Респу́блик, tr. Soyuz Sovetskikh '\n",
      " 'Sotsialisticheskikh Respublik; IPA: [sɐˈjus sɐˈvʲɛtskʲɪx '\n",
      " 'sətsɨəlʲɪsˈtʲitɕɪskʲɪx rʲɪˈspublʲɪk]) abbreviated to USSR (Russian: СССР, '\n",
      " 'tr. SSSR) or shortened to the Soviet Union (Russian: Сове́тский Сою́з, tr. '\n",
      " \"Sovetskij Soyuz; IPA: [sɐ'vʲetskʲɪj sɐˈjʉs]), was a Marxist–Leninist state \"\n",
      " 'on the Eurasian continent that existed between 1922 and 1991.',\n",
      " 'Meantone temperament: Meantone temperament is a musical temperament, which '\n",
      " 'is a system of musical tuning. In general, a meantone is constructed the '\n",
      " 'same way as Pythagorean tuning, as a stack of perfect fifths, but in '\n",
      " 'meantone, each fifth is narrow compared to the ratio 27/12:1 used in 12 '\n",
      " 'equal temperament.',\n",
      " 'Childbirth: Childbirth, also known as labour, delivery, birth, partus, or '\n",
      " 'parturition, is the culmination of a period of pregnancy with the expulsion '\n",
      " \"of one or more newborn infants from a woman's uterus.\",\n",
      " 'Ternary fission: Ternary fission is a comparatively rare (0.2 to 0.4% of '\n",
      " 'events) type of nuclear fission in which three charged products are produced '\n",
      " 'rather than two. As in other nuclear fission processes, other uncharged '\n",
      " 'particles such as multiple neutrons and gamma rays are produced in ternary '\n",
      " 'fission. Ternary fission may happen during neutron-induced fission or in '\n",
      " 'spontaneous fission (the type of radioactive decay).',\n",
      " 'Dallas: Dallas (/ˈdæləs/) is a major city in Texas and is the largest urban '\n",
      " 'center of the fourth most populous metropolitan area in the United States. '\n",
      " 'The city proper ranks ninth in the U.S. and third in Texas after Houston and '\n",
      " \"San Antonio. The city's prominence arose from its historical importance as a \"\n",
      " 'center for the oil and cotton industries, and its position along numerous '\n",
      " 'railroad lines.',\n",
      " 'Genetics: Genetics is the study of genes, heredity, and genetic variation in '\n",
      " 'living organisms. It is generally considered a field of biology, but it '\n",
      " 'intersects frequently with many of the life sciences and is strongly linked '\n",
      " 'with the study of information systems.The father of genetics is Gregor '\n",
      " 'Mendel, a late 19th-century scientist and Augustinian friar. Mendel studied '\n",
      " \"'trait inheritance', patterns in the way traits were handed down from \"\n",
      " 'parents to offspring.',\n",
      " 'Avatar: In Hinduism, an avatar (/ˈævəˌtɑr, ˌævəˈtɑr/; Hindustani: [əʋˈt̪aːr] '\n",
      " 'from Sanskrit अवतार avatāra \"descent\") is a deliberate descent of a deity to '\n",
      " 'Earth, or a descent of the Supreme Being (e.g., Vishnu for Vaishnavites), '\n",
      " 'and is mostly translated into English as \"incarnation\", but more accurately '\n",
      " 'as \"appearance\" or \"manifestation\".The phenomenon of an avatar is observed '\n",
      " 'in Hinduism, Ayyavazhi, and Sikhism.',\n",
      " 'Kindergarten: A kindergarten (German, German pronunciation: [ˈkɪndɐˌgaːtn]), '\n",
      " \"literally children's garden, is a preschool educational approach \"\n",
      " 'traditionally based on playing, singing, practical activities such as '\n",
      " 'drawing, and social interaction as part of the transition from home to '\n",
      " 'school.',\n",
      " 'Wikipedia: Wikipedia (/ˌwɪkɨˈpiːdiə/ or /ˌwɪkiˈpiːdiə/ WIK-i-PEE-dee-ə) is a '\n",
      " 'free-access, free-content Internet encyclopedia, supported and hosted by the '\n",
      " 'non-profit Wikimedia Foundation. Those who can access the site can edit most '\n",
      " 'of its articles. Wikipedia is ranked among the ten most popular websites and '\n",
      " \"constitutes the Internet's largest and most popular general reference \"\n",
      " 'work.Jimmy Wales and Larry Sanger launched Wikipedia on January 15, 2001.',\n",
      " 'Republican Party (United States): The Republican Party, commonly referred to '\n",
      " 'as GOP (abbreviation for Grand Old Party), is one of the two major '\n",
      " 'contemporary political parties in the United States, the other being its '\n",
      " 'historic rival, the Democratic Party.Founded by anti-slavery activists, '\n",
      " 'modernizers, ex-Whigs, and ex-Free Soilers in 1854, the Republicans '\n",
      " 'dominated politics nationally and in the majority of northern States for '\n",
      " 'most of the period between 1860 and 1932.',\n",
      " 'United States: The United States of America (USA), commonly referred to as '\n",
      " 'the United States (U.S.) or America, is a federal republic composed of 50 '\n",
      " 'states, a federal district, five major territories and various possessions. '\n",
      " 'The 48 contiguous states and Washington, D.C., are in central North America '\n",
      " 'between Canada and Mexico. The state of Alaska is located in the '\n",
      " 'northwestern part of North America and the state of Hawaii is an archipelago '\n",
      " 'in the mid-Pacific.',\n",
      " 'Helicopter: A helicopter is a type of rotorcraft in which lift and thrust '\n",
      " 'are supplied by rotors.  This allows the helicopter to take off and land '\n",
      " 'vertically, to hover, and to fly forward, backward, and laterally.',\n",
      " 'Contradiction: In classical logic, a contradiction consists of a logical '\n",
      " 'incompatibility between two or more propositions. It occurs when the '\n",
      " 'propositions, taken together, yield two conclusions which form the logical, '\n",
      " 'usually opposite inversions of each other.',\n",
      " 'Blonde on Blonde: Blonde on Blonde is the seventh studio album by American '\n",
      " 'singer-songwriter Bob Dylan, released on May 16, 1966, on Columbia Records. '\n",
      " 'Recording sessions began in New York in October 1965 with numerous backing '\n",
      " \"musicians, including members of Dylan's live backing band, The Hawks. Though \"\n",
      " 'sessions continued until January 1966, they yielded only one track that made '\n",
      " 'it onto the final album—\"One of Us Must Know (Sooner or Later)\".',\n",
      " 'Endemism: Endemism is the ecological state of a species being unique to a '\n",
      " 'defined geographic location, such as an island, nation, country or other '\n",
      " 'defined zone, or habitat type; organisms that are indigenous to a place are '\n",
      " 'not endemic to it if they are also found elsewhere. The extreme opposite of '\n",
      " 'endemism is cosmopolitan distribution. Another term for a species that is '\n",
      " 'endemic is precinctive, which applies to species (and subspecific '\n",
      " 'categories) that are restricted to a defined geographical area.',\n",
      " 'Yeti: The Yeti (/ˈjɛti/) or Abominable Snowman (Nepali: हिममानव himamānav, '\n",
      " 'lit. \"mountain man\") is an ape-like cryptid taller than an average human '\n",
      " 'that is said to inhabit the Himalayan region of Nepal and Tibet. The names '\n",
      " 'Yeti and Meh-Teh are commonly used by the people indigenous to the region, '\n",
      " 'and are part of their history and mythology.',\n",
      " 'Bible: The Bible (from Koine Greek τὰ βιβλία, tà biblía, \"the books\") is a '\n",
      " 'collection of texts sacred in Judaism and Christianity. There is no single '\n",
      " '\"Bible\" and many Bibles with varying contents exist. Various religious '\n",
      " 'traditions have produced different recensions with different selections of '\n",
      " 'texts.',\n",
      " 'England: England /ˈɪŋɡlənd/ is a country that is part of the United Kingdom. '\n",
      " 'It shares land borders with Scotland to the north and Wales to the west. The '\n",
      " 'Irish Sea lies northwest of England and the Celtic Sea lies to the '\n",
      " 'southwest. England is separated from continental Europe by the North Sea to '\n",
      " 'the east and the English Channel to the south.',\n",
      " 'Certainty: Certainty is perfect knowledge that has total security from '\n",
      " 'error, or the mental state of being without doubt.Objectively defined, '\n",
      " 'certainty is total continuity and validity of all foundational inquiry, to '\n",
      " 'the highest degree of precision. Something is certain only if no skepticism '\n",
      " 'can occur.',\n",
      " 'Vowel: In phonetics, a vowel is a sound in spoken language, such as an '\n",
      " 'English \"ah!\" /ɑː/ or \"oh!\" /oʊ/, pronounced with an open vocal tract so '\n",
      " 'that there is no build-up of air pressure at any point above the glottis. '\n",
      " 'This contrasts with consonants, such as English \"sh!\" [ʃː], which have a '\n",
      " 'constriction or closure at some point along the vocal tract.']\n"
     ]
    }
   ],
   "source": [
    "def get_dbpedia_results(queries):\n",
    "    '''Given a list of query strings, returns a list of result dicts.'''\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        result_for_query = dbpedia_prefix_search(query)\n",
    "        if result_for_query:\n",
    "            results.append(result_for_query[0])\n",
    "    return results\n",
    "\n",
    "#print(get_dbpedia_results(expanded_keywords))\n",
    "\n",
    "def get_dbpedia_result_text(queries):\n",
    "    '''Given a list of query strings, returns a list of (label+description) strings '''\n",
    "    results = get_dbpedia_results(queries)\n",
    "    \n",
    "    results_strings = [str(r['label'] +\": \"+ r['description']) \n",
    "                       for r in results if r['description']]\n",
    "    return list(set(results_strings))\n",
    "\n",
    "db_docs = get_dbpedia_result_text(expanded_keywords)\n",
    "pprint(db_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:23.033570Z",
     "start_time": "2017-09-19T00:07:45.443637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus(59 docs; 4283 tokens)\n"
     ]
    }
   ],
   "source": [
    "def textacy_corpus_dbpedia_results(queries):\n",
    "    '''Given a list of query strings, returns a textacy corpus generated from dbpedia results.'''\n",
    "    corpus = textacy.corpus.Corpus('en', \n",
    "                                 get_dbpedia_result_text(queries), \n",
    "                                 metadatas=get_dbpedia_results(queries))\n",
    "    return corpus\n",
    "\n",
    "#corpus = textacy_corpus_dbpedia_results(expanded_keywords)\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:23.062858Z",
     "start_time": "2017-09-19T00:07:25.315Z"
    }
   },
   "outputs": [],
   "source": [
    "#pos_tagged_blob = extract_pos_tagged_sents_from_corpus(corpus)\n",
    "#print(pos_tagged_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:41.854851Z",
     "start_time": "2017-09-19T00:08:41.826973Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:41.911349Z",
     "start_time": "2017-09-19T00:08:41.857352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['machine word', 0.039718912312862814],\n",
      " ['fruit', 0.026886648334860985],\n",
      " ['category_name', 0.021692636724717385],\n",
      " ['speaking', 0.021387106630003056],\n",
      " ['science', 0.021081576535288724],\n",
      " ['programming', 0.019859456156431407],\n",
      " ['internet', 0.016804155209288116],\n",
      " ['technology', 0.016498625114573784],\n",
      " ['machine_learning data_science', 0.013137794072716162],\n",
      " ['school', 0.012832263978001834],\n",
      " ['communication', 0.011610143599144515],\n",
      " ['work', 0.011610143599144515],\n",
      " ['office', 0.010999083409715857],\n",
      " ['business', 0.010388023220287198],\n",
      " ['messaging', 0.010388023220287198],\n",
      " ['computer', 0.00977696303085854],\n",
      " ['trust', 0.007638252367858234],\n",
      " ['social_media', 0.007332722273143905],\n",
      " ['positive_emotion', 0.006721662083715246],\n",
      " ['college', 0.0058050717995722576],\n",
      " ['meeting', 0.0058050717995722576],\n",
      " ['tool', 0.0058050717995722576],\n",
      " ['politeness', 0.005499541704857928],\n",
      " ['reading', 0.005194011610143599],\n",
      " ['sympathy', 0.00488848151542927],\n",
      " ['phone', 0.00458295142071494],\n",
      " ['shape_and_size', 0.00458295142071494],\n",
      " ['healing', 0.0036663611365719525],\n",
      " ['strength', 0.0036663611365719525],\n",
      " ['journalism', 0.003360831041857623],\n",
      " ['optimism', 0.003360831041857623],\n",
      " ['animal', 0.003360831041857623],\n",
      " ['government', 0.0030553009471432934],\n",
      " ['fashion', 0.0030553009471432934],\n",
      " ['economics', 0.0030553009471432934],\n",
      " ['philosophy', 0.0030553009471432934],\n",
      " ['giving', 0.0030553009471432934],\n",
      " ['sports', 0.002749770852428964],\n",
      " ['appearance', 0.002749770852428964],\n",
      " ['plant', 0.002749770852428964],\n",
      " ['art', 0.002444240757714635],\n",
      " ['leader', 0.002444240757714635],\n",
      " ['hiking', 0.002444240757714635],\n",
      " ['order', 0.002444240757714635],\n",
      " ['children', 0.002444240757714635],\n",
      " ['restaurant', 0.0018331805682859762],\n",
      " ['gain', 0.0018331805682859762],\n",
      " ['writing', 0.0018331805682859762],\n",
      " ['musical', 0.0018331805682859762],\n",
      " ['help', 0.0015276504735716467],\n",
      " ['celebration', 0.0015276504735716467],\n",
      " ['family', 0.0012221203788573174],\n",
      " ['morning', 0.0012221203788573174],\n",
      " ['religion', 0.0012221203788573174],\n",
      " ['heroic', 0.0012221203788573174],\n",
      " ['hearing', 0.0012221203788573174],\n",
      " ['power', 0.0012221203788573174],\n",
      " ['party', 0.0012221203788573174],\n",
      " ['achievement', 0.0012221203788573174],\n",
      " ['feminine', 0.0012221203788573174],\n",
      " ['weapon', 0.0012221203788573174],\n",
      " ['pride', 0.0009165902841429881],\n",
      " ['dispute', 0.0009165902841429881],\n",
      " ['leisure', 0.0009165902841429881],\n",
      " ['wealthy', 0.0009165902841429881],\n",
      " ['play', 0.0009165902841429881],\n",
      " ['hipster', 0.0009165902841429881],\n",
      " ['surprise', 0.0009165902841429881],\n",
      " ['independence', 0.0009165902841429881],\n",
      " ['confusion', 0.0009165902841429881],\n",
      " ['legend', 0.0009165902841429881],\n",
      " ['fabric', 0.0009165902841429881],\n",
      " ['music', 0.0009165902841429881],\n",
      " ['toy', 0.0009165902841429881],\n",
      " ['shopping', 0.0009165902841429881],\n",
      " ['white_collar_job', 0.0009165902841429881],\n",
      " ['disappointment', 0.0009165902841429881],\n",
      " ['negative_emotion', 0.0009165902841429881],\n",
      " ['sleep', 0.0006110601894286587],\n",
      " ['medical_emergency', 0.0006110601894286587],\n",
      " ['hate', 0.0006110601894286587],\n",
      " ['occupation', 0.0006110601894286587],\n",
      " ['envy', 0.0006110601894286587],\n",
      " ['health', 0.0006110601894286587],\n",
      " ['royalty', 0.0006110601894286587],\n",
      " ['stealing', 0.0006110601894286587],\n",
      " ['driving', 0.0006110601894286587],\n",
      " ['worship', 0.0006110601894286587],\n",
      " ['movement', 0.0006110601894286587],\n",
      " ['military', 0.0006110601894286587],\n",
      " ['anonymity', 0.0006110601894286587],\n",
      " ['vehicle', 0.0006110601894286587],\n",
      " ['listen', 0.0006110601894286587],\n",
      " ['injury', 0.0006110601894286587],\n",
      " ['valuable', 0.0006110601894286587],\n",
      " ['emotional', 0.0006110601894286587],\n",
      " ['affection', 0.0006110601894286587],\n",
      " ['traveling', 0.0006110601894286587],\n",
      " ['anger', 0.0006110601894286587],\n",
      " ['politics', 0.0006110601894286587],\n",
      " ['car', 0.0006110601894286587],\n",
      " ['competing', 0.0006110601894286587],\n",
      " ['law', 0.0006110601894286587],\n",
      " ['rural', 0.0006110601894286587],\n",
      " ['colors', 0.0006110601894286587],\n",
      " ['dance', 0.00030553009471432935],\n",
      " ['money', 0.00030553009471432935],\n",
      " ['domestic_work', 0.00030553009471432935],\n",
      " ['cold', 0.00030553009471432935],\n",
      " ['aggression', 0.00030553009471432935],\n",
      " ['vacation', 0.00030553009471432935],\n",
      " ['masculine', 0.00030553009471432935],\n",
      " ['weakness', 0.00030553009471432935],\n",
      " ['suffering', 0.00030553009471432935],\n",
      " ['furniture', 0.00030553009471432935],\n",
      " ['exercise', 0.00030553009471432935],\n",
      " ['home', 0.00030553009471432935],\n",
      " ['divine', 0.00030553009471432935],\n",
      " ['sexual', 0.00030553009471432935],\n",
      " ['childish', 0.00030553009471432935],\n",
      " ['noise', 0.00030553009471432935],\n",
      " ['medieval', 0.00030553009471432935],\n",
      " ['death', 0.00030553009471432935],\n",
      " ['dominant_heirarchical', 0.00030553009471432935],\n",
      " ['exotic', 0.00030553009471432935],\n",
      " ['weather', 0.00030553009471432935],\n",
      " ['ancient', 0.00030553009471432935],\n",
      " ['deception', 0.00030553009471432935],\n",
      " ['fight', 0.00030553009471432935],\n",
      " ['war', 0.00030553009471432935],\n",
      " ['urban', 0.00030553009471432935],\n",
      " ['fire', 0.00030553009471432935],\n",
      " ['sound', 0.00030553009471432935],\n",
      " ['youth', 0.00030553009471432935],\n",
      " ['shame', 0.00030553009471432935],\n",
      " ['clothing', 0.00030553009471432935],\n",
      " ['breaking', 0.00030553009471432935],\n",
      " ['beauty', 0.00030553009471432935],\n",
      " ['timidity', 0.00030553009471432935],\n",
      " ['cleaning', 0.00030553009471432935],\n",
      " ['friends', 0.00030553009471432935],\n",
      " ['payment', 0.00030553009471432935],\n",
      " ['ocean', 0.00030553009471432935],\n",
      " ['wedding', 0.0],\n",
      " ['cheerfulness', 0.0],\n",
      " ['anticipation', 0.0],\n",
      " ['crime', 0.0],\n",
      " ['attractive', 0.0],\n",
      " ['prison', 0.0],\n",
      " ['nervousness', 0.0],\n",
      " ['horror', 0.0],\n",
      " ['swearing_terms', 0.0],\n",
      " ['tourism', 0.0],\n",
      " ['magic', 0.0],\n",
      " ['beach', 0.0],\n",
      " ['banking', 0.0],\n",
      " ['night', 0.0],\n",
      " ['kill', 0.0],\n",
      " ['blue_collar_job', 0.0],\n",
      " ['ridicule', 0.0],\n",
      " ['real_estate', 0.0],\n",
      " ['fear', 0.0],\n",
      " ['irritability', 0.0],\n",
      " ['superhero', 0.0],\n",
      " ['pet', 0.0],\n",
      " ['cooking', 0.0],\n",
      " ['exasperation', 0.0],\n",
      " ['body', 0.0],\n",
      " ['eating', 0.0],\n",
      " ['zest', 0.0],\n",
      " ['water', 0.0],\n",
      " ['violence', 0.0],\n",
      " ['neglect', 0.0],\n",
      " ['swimming', 0.0],\n",
      " ['love', 0.0],\n",
      " ['hygiene', 0.0],\n",
      " ['air_travel', 0.0],\n",
      " ['dominant_personality', 0.0],\n",
      " ['farming', 0.0],\n",
      " ['disgust', 0.0],\n",
      " ['sailing', 0.0],\n",
      " ['rage', 0.0],\n",
      " ['warmth', 0.0],\n",
      " ['sadness', 0.0],\n",
      " ['fun', 0.0],\n",
      " ['joy', 0.0],\n",
      " ['ugliness', 0.0],\n",
      " ['lust', 0.0],\n",
      " ['torment', 0.0],\n",
      " ['ship', 0.0],\n",
      " ['terrorism', 0.0],\n",
      " ['smell', 0.0],\n",
      " ['poor', 0.0],\n",
      " ['pain', 0.0],\n",
      " ['negotiate', 0.0],\n",
      " ['alcohol', 0.0],\n",
      " ['liquid', 0.0],\n",
      " ['monster', 0.0],\n",
      " ['contentment', 0.0],\n",
      " ['proglangs', 0.0],\n",
      " ['umm', 0.0]]\n"
     ]
    }
   ],
   "source": [
    "category_analysis = lexicon.analyze(input_doc, normalize=True, tokenizer='default')\n",
    "\n",
    "top_cats = [[cat[0], cat[1]] for cat in category_analysis.items()]\n",
    "top_cats.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "pprint(top_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create Category from terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.070320Z",
     "start_time": "2017-09-19T00:08:41.913900Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='54.148.189.209', port=8000): Max retries exceeded with url: /create_category (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a77b05f8>: Failed to establish a new connection: [Errno 61] Connection refused',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 141\u001b[0;31m                 (self.host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    600\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 150\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x1a77b05f8>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 )\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    638\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 639\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    640\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='54.148.189.209', port=8000): Max retries exceeded with url: /create_category (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a77b05f8>: Failed to establish a new connection: [Errno 61] Connection refused',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-76b7c30b1568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'''machine, word, language, information, human, style, little, thought, vocabulary, contextual'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlexicon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_category\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"category_name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nytimes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/empath/core.py\u001b[0m in \u001b[0;36mcreate_category\u001b[0;34m(self, name, seeds, model, size, write)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_category\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fiction\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/create_category\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"terms\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"size\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \"\"\"\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    506\u001b[0m         }\n\u001b[1;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='54.148.189.209', port=8000): Max retries exceeded with url: /create_category (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a77b05f8>: Failed to establish a new connection: [Errno 61] Connection refused',))"
     ]
    }
   ],
   "source": [
    "tokens = '''machine, word, language, information, human, style, little, thought, vocabulary, contextual'''\n",
    "tokens = tokens.split(\", \")\n",
    "lexicon.create_category(\"category_name\",tokens, model=\"nytimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:09:42.768113Z",
     "start_time": "2017-09-19T00:09:42.274481Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='54.148.189.209', port=8000): Max retries exceeded with url: /create_category (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x114303c50>: Failed to establish a new connection: [Errno 61] Connection refused',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 141\u001b[0;31m                 (self.host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    600\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 150\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x114303c50>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 )\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    638\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 639\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    640\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='54.148.189.209', port=8000): Max retries exceeded with url: /create_category (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x114303c50>: Failed to establish a new connection: [Errno 61] Connection refused',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-78321d6dd28d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'''machine, word, language, information, human, style, little, thought, vocabulary, contextual'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mexpanded_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategory_from_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fiction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-78321d6dd28d>\u001b[0m in \u001b[0;36mcategory_from_keywords\u001b[0;34m(keywords, model, clean)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mredirect_stdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mlexicon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_category\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mcategory_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mcategory_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategory_terms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#exclude enclosing brackets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/empath/core.py\u001b[0m in \u001b[0;36mcreate_category\u001b[0;34m(self, name, seeds, model, size, write)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_category\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fiction\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/create_category\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"terms\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"size\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \"\"\"\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    506\u001b[0m         }\n\u001b[1;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davideverling/anaconda/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='54.148.189.209', port=8000): Max retries exceeded with url: /create_category (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x114303c50>: Failed to establish a new connection: [Errno 61] Connection refused',))"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import nltk\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def capture_from_stdout(function):\n",
    "    '''Not 100% sure the interior fuction call syntax is correct, but the wrapping is correct'''\n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        function()\n",
    "    return f.getvalue()\n",
    "\n",
    "def category_from_keywords(keywords, model='all', clean=False):\n",
    "    '''Call Empath's create category. Model options: \"fiction\",\"nytimes\",\"reddit\",\"all\"\n",
    "    Returns a list of strings.'''\n",
    "    \n",
    "    if type(keywords) == str:\n",
    "        keywords = map(str.strip, keywords.split(\",\")) #split into individual items\n",
    "    \n",
    "    #replace spaces with underscores for Empath's lexicon format\n",
    "    keywords = [keyword.replace(\" \", \"_\") for keyword in keywords]\n",
    "\n",
    "    category_name = keywords[0] + \" \" + keywords[1] #name the category after the first two keywords\n",
    "    if model  == 'all':\n",
    "        category_terms = []\n",
    "        for model in ['reddit','nytimes','fiction']:\n",
    "            f = io.StringIO()\n",
    "            with redirect_stdout(f):\n",
    "                lexicon.create_category(category_name, keywords, model=model, write=False)\n",
    "            model_terms = f.getvalue().strip().replace(\"[]\",\"\")\n",
    "            if model_terms:\n",
    "                model_terms = model_terms[1:-1] #exclude enclosing brackets\n",
    "                model_terms = model_terms.replace('\"','').split(\", \")\n",
    "                category_terms.append(model_terms)\n",
    "        category_terms = [term for model_terms in category_terms for term in model_terms] #flatten lists\n",
    "        category_terms = [term.replace('_',' ') for term in category_terms] #re-separate on underscores\n",
    "    \n",
    "    else:\n",
    "        f = io.StringIO()\n",
    "        with redirect_stdout(f):\n",
    "            lexicon.create_category(category_name, keywords, model=model)\n",
    "        category_terms = f.getvalue().strip().replace(\"[]\",\"\").replace(\"_\", \" \")\n",
    "        category_terms = category_terms[1:-1] #exclude enclosing brackets\n",
    "        category_terms = category_terms.replace('\"','').split(\", \")\n",
    "\n",
    "    \n",
    "    ### Filter out non-words like urls, unusual characters\n",
    "    if clean == True:\n",
    "        clean_terms = []\n",
    "        for term in category_terms:\n",
    "            clean_terms.append(\" \".join(w for w in nltk.wordpunct_tokenize(term) \\\n",
    "             if w.lower() in words or not w.isalpha()))\n",
    "        category_terms = [term for term in clean_terms if term.isalpha()]\n",
    "    \n",
    "    return category_terms\n",
    "\n",
    "keywords = '''machine, word, language, information, human, style, little, thought, vocabulary, contextual'''\n",
    "expanded_keywords = category_from_keywords(keywords, model='fiction')\n",
    "print(keywords)\n",
    "print(expanded_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.074280Z",
     "start_time": "2017-09-19T00:08:41.878Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_wiki_text_for_keywords(keywords):\n",
    "    all_text = \"\"\n",
    "    if type(keywords)==str:\n",
    "        keywords = keywords.split(\",\")\n",
    "        \n",
    "    print(keywords)    \n",
    "    for keyword in keywords:\n",
    "        wiki_data = get_wiki_page(keyword, summary=True, content=True)\n",
    "        if wiki_data != None:\n",
    "            try:\n",
    "                #summary = wiki_data['summary']\n",
    "                content = wiki_data['content']\n",
    "                all_text += content\n",
    "            except KeyError:\n",
    "                continue\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "wiki_blob = get_wiki_text_for_keywords(keywords)\n",
    "print(wiki_blob)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Category from Key Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.075686Z",
     "start_time": "2017-09-19T00:08:41.897Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_abstract = '''This research looks at the work of Margaret C. Anderson, the editor of the Little Review.  The review published first works by Sherwood Anderson, James Joyce, Wyndham Lewis, and Ezra Pound.  This research draws upon mostly primary sources including memoirs, published letters, and a complete collection of the Little Review. Most prior research on Anderson focuses on her connection to the famous writers and personalities that she published and associated with.  This focus undermines her role as the dominant creative force behind one of the most influential little magazines published in the 20th Century. This case example shows how little magazine publishing is arguably a literary art.'''\n",
    "keyterms = get_textacy_key_terms(to_textacy_doc(sample_abstract))\n",
    "print(\"Extracted Key Terms:\\n\", keyterms)\n",
    "sample_abstract_cat = category_from_keywords([keyterm[0] for keyterm in keyterms])\n",
    "print(\"Generated Category Words:\\n\", sample_abstract_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T17:59:10.873708Z",
     "start_time": "2017-09-12T17:59:10.866202Z"
    }
   },
   "source": [
    "## Markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T01:31:27.325972Z",
     "start_time": "2017-09-19T01:31:27.106614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This project involves discovering how the American Revolution lives on in memory, this research highlights the importance of the United States government during the 1800s.\n",
      "This project involves discovering how the American Revolution lives on in memory, this research highlights the importance of the United States government during the nineteenth century.\n",
      "Through showing that the American Revolution was remembered during the 1800s.\n",
      "The goal is to show that the American Revolution was remembered during the nineteenth century.\n",
      "This project involves discovering how the American Revolution lives on in memory, this research highlights the importance of the United States government during the 1800s.\n"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "text = raw_abstracts[1]\n",
    "\n",
    "# Build the model.\n",
    "text_model = markovify.Text(text)\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "for i in range(5):\n",
    "    print(text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tagger class using spaCy components for fast POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.078417Z",
     "start_time": "2017-09-19T00:08:41.928Z"
    }
   },
   "outputs": [],
   "source": [
    "import markovify\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "# corpus_path = \"/corpus_data/comp_ling.txt\"\n",
    "# corpus_name = \"comp_ling\"\n",
    "#corpus = textacy.Corpus('en', [raw_abstracts[1], \" \"])\n",
    "#corpus = textacy.Corpus.add_doc(textacy.Corpus('en'), to_textacy_doc(wiki_blob))\n",
    "#corpus = wiki_blob_spacy\n",
    "#print(corpus)\n",
    "\n",
    "class TaggedText(markovify.Text):\n",
    "\n",
    "    def sentence_split(self, text):\n",
    "        \"\"\"\n",
    "        Splits full-text string into a list of sentences.\n",
    "        \"\"\"\n",
    "        sentence_list = []\n",
    "        for doc in corpus:\n",
    "            sentence_list += list(doc.sents)\n",
    "\n",
    "        return sentence_list\n",
    "\n",
    "    def word_split(self, sentence):\n",
    "        \"\"\"\n",
    "        Splits a sentence into a list of words.\n",
    "        \"\"\"\n",
    "        #print(sentence)\n",
    "        return [\"::\".join((word.orth_,word.pos_)) for word in sentence]\n",
    "\n",
    "    def word_join(self, words):\n",
    "        sentence = \" \".join(word.split(\"::\")[0] for word in words)\n",
    "        return sentence\n",
    "\n",
    "    def test_sentence_input(self, sentence):\n",
    "        \"\"\"\n",
    "        A basic sentence filter. This one rejects sentences that contain\n",
    "        the type of punctuation that would look strange on its own\n",
    "        in a randomly-generated sentence. \n",
    "        \"\"\"\n",
    "        sentence = sentence.text\n",
    "        reject_pat = re.compile(r\"(^')|('$)|\\s'|'\\s|[\\\"(\\(\\)\\[\\])]\")\n",
    "        # Decode unicode, mainly to normalize fancy quotation marks\n",
    "        if sentence.__class__.__name__ == \"str\":\n",
    "            decoded = sentence\n",
    "        else:\n",
    "            decoded = unidecode(sentence)\n",
    "        # Sentence shouldn't contain problematic characters\n",
    "        if re.search(reject_pat, decoded): return False\n",
    "        return True\n",
    "\n",
    "    def generate_corpus(self, text):\n",
    "        \"\"\"\n",
    "        Given a text string, returns a list of lists; that is, a list of\n",
    "        \"sentences,\" each of which is a list of words. Before splitting into \n",
    "        words, the sentences are filtered through `self.test_sentence_input`\n",
    "        \"\"\"\n",
    "        sentences = self.sentence_split(text)\n",
    "        passing = filter(self.test_sentence_input, sentences)\n",
    "        runs = list(map(self.word_split, passing))\n",
    "        #print(runs[:10])\n",
    "        return runs\n",
    "\n",
    "# Generated the model\n",
    "model = TaggedText(corpus, state_size=2)\n",
    "# A sentence based on the model\n",
    "print(model.make_sentence())\n",
    "model.make_short_sentence(max_chars=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Sentence Starting With..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.080022Z",
     "start_time": "2017-09-19T00:08:41.941Z"
    }
   },
   "outputs": [],
   "source": [
    "### Using default Markovify model\n",
    "text = raw_abstracts[1]\n",
    "text_model = markovify.Text(text, state_size=1)\n",
    "try:\n",
    "    for i in range(5):\n",
    "        print(text_model.make_sentence_with_start(\"importance\"))\n",
    "except KeyError:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.081685Z",
     "start_time": "2017-09-19T00:08:41.958Z"
    }
   },
   "outputs": [],
   "source": [
    "### Using spaCy-fied POS parsed model\n",
    "model = TaggedText(corpus, state_size=1)\n",
    "try:\n",
    "    start_with_token = \"importance\"\n",
    "    for i in range(5):\n",
    "        print(model.make_sentence_with_start(nlp(start_with_token)))\n",
    "except KeyError:\n",
    "    None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.083216Z",
     "start_time": "2017-09-19T00:08:41.971Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_markov_completions(partial_sentence, trained_markovify_model, n_completions=10):\n",
    "    spacy_sentence = nlp(partial_sentence) #convert to spaCy doc\n",
    "    last_word = str(spacy_sentence[-1])\n",
    "    #last_words = str(spacy_sentence[-2]) + \" \" + last_word\n",
    "\n",
    "    completions = []\n",
    "    for completion in range(n_completions):\n",
    "        completions.append(trained_markovify_model.make_sentence_with_start(last_word))\n",
    "        \n",
    "    return list(set(completions))\n",
    "\n",
    "try:\n",
    "    generate_markov_completions(\"Overwatch\", model)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T18:01:23.594143Z",
     "start_time": "2017-09-12T18:01:23.591123Z"
    }
   },
   "source": [
    "## Synonym Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Wordnet Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.088577Z",
     "start_time": "2017-09-19T00:08:41.983Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "for i,j in enumerate(wn.synsets('java')):\n",
    "    print(\"Meaning\",i, \"NLTK ID:\", j.name())\n",
    "    print(\"Definition:\",j.definition())\n",
    "    print(\"Synonyms:\", \", \".join(j.lemma_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.091269Z",
     "start_time": "2017-09-19T00:08:41.999Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for ss in wn.synsets('small'):\n",
    "    print(ss, ss.examples())\n",
    "    for sim in ss.similar_tos():\n",
    "        print('    {}'.format(sim))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.092926Z",
     "start_time": "2017-09-19T00:08:42.013Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "for i,j in enumerate(wn.synsets('computer')):\n",
    "    print(\"Meaning\",i, \"NLTK ID:\", j.name())\n",
    "    hypernyms = list(chain(*[l.lemma_names() for l in j.hypernyms()]))\n",
    "    hyponyms = list(chain(*[l.lemma_names() for l in j.hyponyms()]))\n",
    "    print(\"Hypernyms:\", \", \".join(hypernyms))\n",
    "    print(\"Hyponyms:\", \", \".join(hyponyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.093631Z",
     "start_time": "2017-09-19T00:08:42.028Z"
    }
   },
   "outputs": [],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "def dict_from_doc_tokens(unigram_tokens):\n",
    "    '''Calls PyDictionary (which calls thesaurus.com) to retrieve synonyms'''\n",
    "    pydict=PyDictionary(unigram_tokens)\n",
    "    \n",
    "    dictionary = defaultdict(dict)\n",
    "    for word in unigram_tokens:\n",
    "        meaning = pydict.meaning(word)\n",
    "        synonyms = pydict.synonym(word)\n",
    "        dictionary[word] = {\"meaning\":meaning, \"synonyms\":synonyms}\n",
    "    return dictionary\n",
    "\n",
    "def get_meanings(word, pos=\"all\"):\n",
    "    '''Returns meaning definitions from an existing pydictionary, or None if no meanings found'''\n",
    "    meanings = dictionary[word]['meaning']\n",
    "\n",
    "    if pos.startswith(\"all\"):\n",
    "        return meanings\n",
    "    if pos.startswith(\"N\"):\n",
    "        return meanings['Noun']\n",
    "    if pos.startswith(\"V\"):\n",
    "        return meanings[\"Verb\"]\n",
    "    if pos.startswith(\"J\"):\n",
    "        return meanings[\"Adjective\"]\n",
    "    if pos.startswith(\"RB\"):\n",
    "        return meanings[\"Adverb\"]\n",
    "    \n",
    "    return meanings\n",
    "\n",
    "tokens = ['alabaster']\n",
    "dictionary = dict_from_doc_tokens(tokens)\n",
    "pprint(get_meanings(tokens[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Proselint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.094388Z",
     "start_time": "2017-09-19T00:08:42.047Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import proselint\n",
    "from proselint.tools import errors_to_json\n",
    "\n",
    "suggestions = proselint.tools.lint(\"this is a very unique sentence\")\n",
    "errors_to_json(suggestions)\n",
    "\n",
    "# for suggestion in suggestions:\n",
    "#         check = suggestion[0]\n",
    "#         message = suggestion[1]\n",
    "#         line = suggestion[2]\n",
    "#         column = suggestion[3]\n",
    "#         start = suggestion[4]\n",
    "#         end = suggestion[5]\n",
    "#         extent = suggestion[6]\n",
    "#         severity = suggestion[7]\n",
    "#         replacements = suggestion[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.097651Z",
     "start_time": "2017-09-19T00:08:42.062Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import proselint\n",
    "from proselint.tools import errors_to_json\n",
    "import json\n",
    "\n",
    "def linter_suggestions(text):\n",
    "    ''' Returns suggestions as a list of dicts. Each dict is a suggestion with the following properties:\n",
    "    (check, message, line, column, start, end, extent, severity, replacements)\n",
    "    '''\n",
    "    suggestions = proselint.tools.lint(text)\n",
    "    json_string = errors_to_json(suggestions) \n",
    "    json_dict = json.loads(json_string)\n",
    "    return json_dict['data']['errors']\n",
    "\n",
    "linter_suggestions(\"and then I said... there goes a very unique thing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T02:44:08.406450Z",
     "start_time": "2017-09-12T02:44:08.345351Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## TextGenRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.100571Z",
     "start_time": "2017-09-19T00:08:42.081Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from textgenrnn import textgenrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.122174Z",
     "start_time": "2017-09-19T00:08:42.109Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "textgen = textgenrnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.134544Z",
     "start_time": "2017-09-19T00:08:42.126Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "textgen.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.144716Z",
     "start_time": "2017-09-19T00:08:42.143Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "generated_texts = textgen.generate(n=5, prefix=\"Machine learning\", temperature=0.2, return_as_list=True)\n",
    "pprint(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.158071Z",
     "start_time": "2017-09-19T00:08:42.156Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts = [input_doc]\n",
    "\n",
    "textgen.train_on_texts(texts, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.170893Z",
     "start_time": "2017-09-19T00:08:42.167Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"---\\nNormal Output\\n---\")\n",
    "textgen.generate(5)\n",
    "print(\"---\\nHigh Temperature Output\\n---\")\n",
    "textgen.generate(5, temperature=1.0)\n",
    "print(\"---\\nPrefix Output\\n---\")\n",
    "textgen.generate(5, prefix=\"N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.189490Z",
     "start_time": "2017-09-19T00:08:42.187Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.200840Z",
     "start_time": "2017-09-19T00:08:42.198Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_summary(text, ratio=0.25):\n",
    "    '''Wraps gensim summarize()'''\n",
    "    return summarize(text, ratio)\n",
    "extract_summary(input_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.213944Z",
     "start_time": "2017-09-19T00:08:42.212Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "keywords(input_doc).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-18T23:45:24.408292Z",
     "start_time": "2017-09-18T23:45:24.310704Z"
    }
   },
   "source": [
    "## Sentence completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.229138Z",
     "start_time": "2017-09-19T00:08:42.227Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sentence Sources\n",
    "#dbpedia\n",
    "#PyDictionary (most flexible, but makes web call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.243593Z",
     "start_time": "2017-09-19T00:08:42.241Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def extract_keywords(text):\n",
    "#    '''Wraps textacy keywords function, returns a list of keyword strings'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.257450Z",
     "start_time": "2017-09-19T00:08:42.255Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def category_from_keywords(keywords, model='all', clean=False):\n",
    "#'''Call Empath's create category. Model options: \"fiction\",\"nytimes\",\"reddit\",\"all\"\n",
    "#    Returns a list of strings.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.277626Z",
     "start_time": "2017-09-19T00:08:42.275Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def textacy_corpus_dbpedia_results(queries):\n",
    "#    '''Given a list of query strings, returns a textacy corpus generated from dbpedia results.'''\n",
    "#    corpus = textacy.corpus.Corpus('en', \n",
    "#                                 get_dbpedia_result_text(queries), \n",
    "#                                 metadatas=get_dbpedia_results(queries))\n",
    "#    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.292413Z",
     "start_time": "2017-09-19T00:08:42.290Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_sentences(doc):\n",
    "#     '''Returns a list of spacy spans.'''\n",
    "#     if not isinstance(doc, textacy.doc.Doc):\n",
    "#         doc = to_textacy_doc(doc)\n",
    "#     return list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:56:00.259549Z",
     "start_time": "2017-09-19T00:56:00.254023Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dbpedia_result_text(queries):\n",
    "    '''Given a list of query strings, returns a list of (label+description) strings '''\n",
    "    results = get_dbpedia_results(queries)\n",
    "    \n",
    "    results_strings = [str(r['label'] +\": \"+ r['description']) \n",
    "                       for r in results if r['description']]\n",
    "    return list(set(results_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T02:42:22.087955Z",
     "start_time": "2017-09-19T02:42:22.083815Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = '''Metis is a cool data science bootcamp immersive program where I learned a ton about machine learning, natural language processing, probability, and statistics. Natural Language Processing is cool. Data Science is a burgeoning field. President Obama is cool.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T02:43:59.616232Z",
     "start_time": "2017-09-19T02:43:59.159613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cool', 'science', 'natural', 'processing', 'program', 'ton', 'immersive', 'machine', 'field', 'president']\n",
      "['Natural Language Processing', 'Data Science', 'Obama']\n",
      "Corpus(12 docs; 889 tokens)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Barack Obama: Barack Hussein Obama II (US /bəˈrɑːk huːˈseɪn ɵˈbɑːmə/; born August 4, 1961) is the 44th and current President of the United States, and the first African American to hold the office. Born in Honolulu, Hawaii, Obama is a graduate of Columbia University and Harvard Law School, where he served as president of the Harvard Law Review. He was a community organizer in Chicago before earning his law degree.',\n",
       " 'Data science: Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from large volumes of data in various forms, either structured or unstructured, which is a continuation of some of the data analysis fields such as statistics, data mining and predictive analytics, as well as as Knowledge Discovery in Databases (KDD).',\n",
       " 'Natural language processing: Natural language processing (NLP) is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of human–computer interaction. Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "doc = to_textacy_doc(text)\n",
    "\n",
    "keywords = get_semantic_key_terms(doc)\n",
    "print(keywords)\n",
    "\n",
    "entities = [str(ent) for ent in textacy.extract.named_entities(doc)]\n",
    "print(entities)\n",
    "\n",
    "knowledge = textacy_corpus_dbpedia_results(keywords+entities)\n",
    "print(knowledge)\n",
    "def get_completions(doc):\n",
    "    '''Accepts string or textacy doc. Returns list of strings.'''\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    \n",
    "    ents = [str(ent) for ent in textacy.extract.named_entities(doc)]   \n",
    "    completions = get_dbpedia_result_text(ents)    \n",
    "        \n",
    "        \n",
    "        \n",
    "    return completions\n",
    "\n",
    "get_completions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T02:35:18.424537Z",
     "start_time": "2017-09-19T02:35:18.395585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural language processing: Natural language processing (NLP) is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of human–computer interaction. Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.',\n",
       " 'Dog: The domestic dog (Canis lupus familiaris or Canis familiaris) is a domesticated canid which has been selectively bred for millennia for various behaviors, sensory capabilities, and physical attributes.Although initially thought to have originated as a manmade variant of an extant canid species (variously supposed as being the dhole, golden jackal, or gray wolf), extensive genetic studies undertaken during the 2010s indicate that dogs diverged from other wolf-like canids in Eurasia 40,000 years ago. ']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dbpedia_result_text([\"Natural Language Processing\", \"dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T02:48:21.695035Z",
     "start_time": "2017-09-19T02:48:21.685374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Metis, is, immersive program), (I, learned, ton), (Data Science, is, field)]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(textacy.extract.subject_verb_object_triples(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
