{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:43.641444Z",
     "start_time": "2017-09-19T00:05:43.636707Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T22:40:23.448248Z",
     "start_time": "2017-09-12T22:40:23.437329Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Quill stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:43.650161Z",
     "start_time": "2017-09-19T00:05:43.645912Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_delta = '''{\"ops\":[\n",
    "  { \"insert\": \"#\" },\n",
    "  { \"insert\": \"!\", \"attributes\": { \"bold\": \"true\" }}\n",
    "]}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:43.690226Z",
     "start_time": "2017-09-19T00:05:43.667262Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def read_Delta(Delta):\n",
    "    '''Expects a list of operations (dicts)'''\n",
    "    if type(Delta) == str:\n",
    "        Delta = json.loads(Delta)\n",
    "    \n",
    "    for op_dict in Delta[\"ops\"]:\n",
    "        for op, value in op_dict.items():\n",
    "            print(value)\n",
    "            if (op == 'insert') and (value==\"#\"): \n",
    "                hashtag = True\n",
    "    return Delta\n",
    "\n",
    "read_Delta(test_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy and TextaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:50.825535Z",
     "start_time": "2017-09-19T00:05:43.696005Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def to_spacy_doc(raw_doc):\n",
    "    '''Converts a raw string into a spaCy document'''\n",
    "    return nlp(raw_doc)\n",
    "\n",
    "def to_textacy_doc(raw_doc):\n",
    "    '''Converts a raw string into a spaCy doc, then a textacy doc'''\n",
    "    if isinstance(to_spacy_doc(\"test\"), spacy.tokens.doc.Doc):\n",
    "        return textacy.Doc(raw_doc)\n",
    "    else:\n",
    "        return textacy.Doc(nlp(raw_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample input documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:52.160310Z",
     "start_time": "2017-09-19T00:05:50.836824Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Setup testing documents\n",
    "doc_path = 'corpus_data/comp_ling.txt'\n",
    "\n",
    "with open(doc_path) as f:\n",
    "    input_doc = f.read()\n",
    "\n",
    "doc=to_spacy_doc(input_doc)\n",
    "\n",
    "autism_abstract = '''The purpose of this research is to identify a subtype of autism called Developmental Verbal Dyspraxia (DVD).  DVD is a motor-speech problem, disabling oral-motor movements needed for speaking. The first phase of the project involves a screening interview where we identify DVD and Non-DVD kids.  We also use home videos to validate answers on the screening interview.  The final phase involves home visits where we use several assessments to confirm the child’s diagnosis and examine the connection between manual and oral motor challenges. By identifying DVD as a subtype of Autism, we will eliminate the assumption that all Autistics have the same characteristics. This will allow for more individual consideration of Autistic people and may direct future research on the genetic factors in autism.'''\n",
    "history_abstract = '''This project involves discovering how the American Revolution was remembered during the nineteenth century.  The goal is to show that the American Revolution was memorialized by the actions of the United States government during the 1800s. This has been done by examining events such as the Supreme Court cases of John Marshall and the Nullification Crisis. Upon examination of these events, it becomes clear that John Marshall and John Calhoun (creator of the Doctrine of Nullification) attempted to use the American Revolution to bolster their claims by citing speeches from Founding Fathers. Through showing that the American Revolution lives on in memory, this research highlights the importance of the revolution in shaping the actions of the United States government.'''\n",
    "games_abstract = '''The study is to show how even a “sport” video game can incorporate many types of learning, to call attention to what might be overlooked as significant forms of learning, and to understand and take advantage of the opportunities video games afford as more deliberate learning environments. The aspects explored are the skills and techniques required to be successful in the game, the environment that skaters skate in, the personal vs. group identity that is shown through the general appearance of the skater, and the values and icons that the game teaches players. We are finding that sport video games support learning; we hope to find how one learns about oneself as a learner from playing.'''\n",
    "\n",
    "raw_abstracts = [autism_abstract, history_abstract, games_abstract]\n",
    "abstracts = list(map(to_textacy_doc, raw_abstracts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy parse as Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:52.408269Z",
     "start_time": "2017-09-19T00:05:52.163616Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### view how spacy parses doc\n",
    "import pandas as pd\n",
    "\n",
    "token_attributes = [(token.orth_,\n",
    "                     token.prob,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num,\n",
    "                     token.is_oov)\n",
    "                    for token in doc]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'log_probability',\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           'out of vocab.?'])\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Vector Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-20T03:42:33.347235Z",
     "start_time": "2017-09-20T03:42:31.519843Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similar_to_target_by_brown_cluster(words_to_compare, target_word):\n",
    "    similar_words = []\n",
    "    for word_to_compare in words_to_compare:\n",
    "        if abs(nlp.vocab[word_to_compare].cluster - nlp.vocab[target_word].cluster) < 1:\n",
    "            similar_words.append(word_to_compare)\n",
    "    return similar_words\n",
    "\n",
    "similar_to_target_by_brown_cluster(list(nlp.vocab.strings), 'show')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:52.434091Z",
     "start_time": "2017-09-19T00:05:52.426834Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = input_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:54.822288Z",
     "start_time": "2017-09-19T00:05:52.437284Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words(doc):\n",
    "    '''Gets the word tokens for a textacy document, excluding stopwords and punc'''\n",
    "    if type(doc) == str:\n",
    "        doc = to_textacy_doc(doc)\n",
    "    return list(textacy.extract.words(doc))\n",
    "\n",
    "#print(get_words(input_doc))\n",
    "\n",
    "def get_content_words(doc):\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    return list(textacy.extract.words(doc, filter_stops=True, filter_punct=True, filter_nums=False, include_pos=None, exclude_pos=None, min_freq=1))\n",
    "print(get_content_words(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:49:27.733469Z",
     "start_time": "2017-09-19T00:49:27.705594Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_named_entities(doc):\n",
    "    nes = textacy.extract.named_entities(doc)\n",
    "    return [ne for ne in nes]\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    '''Given a text document, extracts named entities using spaCy and builds a dict of metadata for each.\n",
    "    \n",
    "    Example Entity Type Labels:\n",
    "    ORGANIZATION\tGeorgia-Pacific Corp., WHO\n",
    "    PERSON\tEddy Bonte, President Obama\n",
    "    LOCATION\tMurray River, Mount Everest\n",
    "    DATE\tJune, 2008-06-29\n",
    "    TIME\ttwo fifty a m, 1:30 p.m.\n",
    "    MONEY\t175 million Canadian Dollars, GBP 10.40\n",
    "    PERCENT\ttwenty pct, 18.75 %\n",
    "    FACILITY\tWashington Monument, Stonehenge\n",
    "    GPE\tSouth East Asia, Midlothian\n",
    "    '''\n",
    "    \n",
    "    doc = to_spacy_doc(text)\n",
    "    named_entities = defaultdict(dict)\n",
    "    for ent in doc.ents:\n",
    "        ent_name = ent.text\n",
    "        named_entities[ent_name]['label'] = ent.label_\n",
    "        named_entities[ent_name]['text'] = ent.text\n",
    "        wiki_url = None #get_wiki_page(str(ent))['url']\n",
    "        if wiki_url:\n",
    "            named_entities[ent_name]['url'] = wiki_url\n",
    "        \n",
    "    return named_entities\n",
    "extract_named_entities(\"Paul is a man. Jane is a woman.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Readability Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:55.494332Z",
     "start_time": "2017-09-19T00:05:54.854211Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_readability_stats(doc):\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    ts = textacy.text_stats.TextStats(doc)\n",
    "    return ts.readability_stats\n",
    "\n",
    "get_readability_stats(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:05:55.592012Z",
     "start_time": "2017-09-19T00:05:55.497166Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentences_readability(sentences):\n",
    "    try:\n",
    "        sentence_stats = [get_readability_stats(str(sent)) for sent in sentences]\n",
    "        return sentence_stats\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "#sentences_readability(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:06:27.395980Z",
     "start_time": "2017-09-19T00:06:27.090961Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(doc):\n",
    "    '''Returns a list of spacy spans.'''\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    return list(doc.sents)\n",
    "\n",
    "sentences = get_sentences(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Key Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get key terms from semantic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:53:21.709244Z",
     "start_time": "2017-09-19T00:53:21.517514Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_semantic_key_terms(doc, top_n_terms=10, filtered=True):\n",
    "    '''Gets key terms from semantic network. '''\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    term_prob_pairs = textacy.keyterms.key_terms_from_semantic_network(doc)\n",
    "    max_keyterm_weight = term_prob_pairs[0][1]\n",
    "    \n",
    "    # keep keyterms if they're at least half as important as the most important keyterm\n",
    "    # term[0] is the word, term[1] is its keyterm-ness.\n",
    "    if filtered:\n",
    "        terms = [[term[0], term[1]] for term in term_prob_pairs if term[1] >= 0.5*(max_keyterm_weight)]\n",
    "    else:\n",
    "        terms = term_prob_pairs\n",
    "    \n",
    "    #textacy.keyterms.aggregate_term_variants(terms) #aggregates terms that are variations of each other\n",
    "    \n",
    "    return [term[0] for term in terms[:top_n_terms]]\n",
    "\n",
    "get_semantic_key_terms(to_textacy_doc(raw_abstracts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Extract Keyterms with SGRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:06:27.494564Z",
     "start_time": "2017-09-19T00:06:27.448409Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#textacy.keyterms.sgrank(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Keyterms with TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.392763Z",
     "start_time": "2017-09-19T00:07:08.389692Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#textacy.keyterms.textrank(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Extract acronyms and their definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.403657Z",
     "start_time": "2017-09-19T00:07:08.399477Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc = abstracts[0]\n",
    "sample_abstract = raw_abstracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.433716Z",
     "start_time": "2017-09-19T00:07:08.413423Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "textacy.extract.acronyms_and_definitions(to_textacy_doc(sample_abstract))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Extract semantic chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-20T01:47:04.400115Z",
     "start_time": "2017-09-20T01:47:04.337969Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(textacy.extract.noun_chunks(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.450531Z",
     "start_time": "2017-09-19T00:07:08.437565Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Semi-structured statements\n",
    "doc = abstracts[1]\n",
    "print(doc)\n",
    "list(textacy.extract.semistructured_statements(doc, \"project\", cue=u'involve'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.465877Z",
     "start_time": "2017-09-19T00:07:08.457555Z"
    }
   },
   "outputs": [],
   "source": [
    "### Extract Subject-Verb-Object Triples\n",
    "list(textacy.extract.subject_verb_object_triples(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.477963Z",
     "start_time": "2017-09-19T00:07:08.468414Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_pos_tagged_sents_from_corpus(textacy_corpus):\n",
    "    '''Returns a list of documents, each composed of list of sentences.\n",
    "    Sentences are lists of tuples of the form (token, POS)'''\n",
    "    return [doc.pos_tagged_text for doc in textacy_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.516016Z",
     "start_time": "2017-09-19T00:07:08.480675Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def extract_verbs(doc):\n",
    "    '''Returns a list of strings that are verb-tagged tokens.'''\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    all_token_pos_pairs = itertools.chain(*doc.pos_tagged_text) #flatten list\n",
    "    verbs = [token for token, pos in all_token_pos_pairs if pos.startswith(\"V\")]\n",
    "    return verbs\n",
    "print(extract_verbs(doc))\n",
    "\n",
    "def bag_of_words(doc, as_strings=True):\n",
    "    '''Returns a dictionary with word:count pairs. Words are grouped by lemma.'''\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    return doc.to_bag_of_words(as_strings=as_strings)\n",
    "      \n",
    "pprint(bag_of_words(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:58:27.722628Z",
     "start_time": "2017-09-12T19:58:27.712079Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Wikipedia wrapper (API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.646294Z",
     "start_time": "2017-09-19T00:07:08.526459Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from wikipedia import DisambiguationError, PageError, RedirectError\n",
    "\n",
    "def get_wiki_page(search_string, summary=False, content=False):\n",
    "    '''Returns results from searching for search_string with wikipedia wrapper library. \n",
    "       Note: Makes a web request'''\n",
    "    try:\n",
    "        page = wikipedia.page(search_string)\n",
    "        page_data = {\"url\":page.url,\n",
    "                     \"title\":page.title}\n",
    "        if content:\n",
    "            page_data[\"content\"] = page.content # Full text content of page.\n",
    "        if summary:\n",
    "            page_data[\"summary\"] = page.summary # Summary section only.\n",
    "    \n",
    "    except DisambiguationError as e:\n",
    "        return get_wiki_page(e.options[0]) #naively choose first option\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "    return page_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.655811Z",
     "start_time": "2017-09-19T00:07:08.648557Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_doc = \"London\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.707893Z",
     "start_time": "2017-09-19T00:07:08.659514Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import bleach\n",
    "\n",
    "def set_link_title(attrs, new=False):\n",
    "    attrs[(None, u'title')] = u'AI-provided Link'\n",
    "    return attrs\n",
    "\n",
    "def linkify(string):\n",
    "    '''Calls bleach.linkify.\n",
    "    Converts urls in the input string into links. \n",
    "    Returns a string of HTML.'''\n",
    "    if type(string) != str:\n",
    "        raise TypeError(\"input should be a string\")\n",
    "        \n",
    "    linker = Linker(callbacks=[set_link_title])\n",
    "    return bleach.linkify(string)\n",
    "\n",
    "def create_hyperlink(url, display_text, attrs=\"\"):\n",
    "    '''Optional attrs is a string of tag attributes.\n",
    "       Example call:\n",
    "       create_hyperlink('www.google.com', 'Google', \n",
    "       ... attrs = 'class=link_class title=\"Custom Title\"')'''\n",
    "    hyperlink_format = '<a href=\"{link}\" {attrs}>{text}</a>'\n",
    "    return hyperlink_format.format(link=url, attrs=attrs, text=display_text)\n",
    "\n",
    "create_hyperlink('www.google.com', 'Google', attrs='class=link_class title=\"Custom Title\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.709016Z",
     "start_time": "2017-09-19T00:07:08.493Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### HTML-related functions\n",
    "def create_hyperlink(url, display_text, attrs=\"\"):\n",
    "    '''Optional attrs is a string of tag attributes.\n",
    "       \n",
    "       create_hyperlink('www.google.com', 'Google', \n",
    "       ... attrs = 'class=link_class title=\"Custom Title\"')'''\n",
    "    \n",
    "    hyperlink_format = '<a href=\"{link}\" {attrs}>{text}</a>'\n",
    "    return hyperlink_format.format(link=url, attrs=attrs, text=display_text)\n",
    "\n",
    "def linkify_entity(ent_dict):\n",
    "    '''Operates on extracted named entities. Returns HTML string.'''\n",
    "    ent_type = ent_dict['label']\n",
    "    text = ent_dict['text'] \n",
    "    url = get_wiki_page(text)['url']\n",
    "    attrs = 'class=\"{ent_type}\" title=\"{text}\"'.format(ent_type=ent_type, text=text)\n",
    "    return create_hyperlink(url, text, attrs=attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.710515Z",
     "start_time": "2017-09-19T00:07:08.501Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_doc = \"London is a city in the U.K.\"\n",
    "london = extract_named_entities(test_doc)[\"London\"]\n",
    "print(london)\n",
    "print(linkify_entity(london))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Wiktionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.711473Z",
     "start_time": "2017-09-19T00:07:08.513Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from wiktionaryparser import WiktionaryParser, WikiParse\n",
    "parser = WiktionaryParser()\n",
    "word = parser.fetch('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Wikipedia XML Dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T06:45:16.997470Z",
     "start_time": "2017-09-13T06:45:16.994459Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### http://www.heatonresearch.com/2017/03/03/python-basic-wikipedia-parsing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.712190Z",
     "start_time": "2017-09-19T00:07:08.541Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as etree\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "PATH_WIKI_XML = '/Users/davideverling/Projects/Data'\n",
    "FILENAME_WIKI = 'enwiki-latest-pages-articles.xml'\n",
    "FILENAME_ARTICLES = 'articles.csv'\n",
    "FILENAME_REDIRECT = 'articles_redirect.csv'\n",
    "FILENAME_TEMPLATE = 'articles_template.csv'\n",
    "ENCODING = \"utf-8\"\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "def strip_tag_name(t):\n",
    "    t = elem.tag\n",
    "    idx = k = t.rfind(\"}\")\n",
    "    if idx != -1:\n",
    "        t = t[idx + 1:]\n",
    "    return t\n",
    "\n",
    "pathWikiXML = os.path.join(PATH_WIKI_XML, FILENAME_WIKI)\n",
    "pathArticles = os.path.join(PATH_WIKI_XML, FILENAME_ARTICLES)\n",
    "pathArticlesRedirect = os.path.join(PATH_WIKI_XML, FILENAME_REDIRECT)\n",
    "pathTemplateRedirect = os.path.join(PATH_WIKI_XML, FILENAME_TEMPLATE)\n",
    "\n",
    "totalCount = 0\n",
    "articleCount = 0\n",
    "redirectCount = 0\n",
    "templateCount = 0\n",
    "title = None\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:08.714575Z",
     "start_time": "2017-09-19T00:07:08.560Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''Begin streaming the XML file and \n",
    "write the headers for the 3 CSV files that will be built according to the data found in the XML.'''\n",
    "\n",
    "with codecs.open(pathArticles, \"w\", ENCODING) as articlesFH, \\\n",
    "        codecs.open(pathArticlesRedirect, \"w\", ENCODING) as redirectFH, \\\n",
    "        codecs.open(pathTemplateRedirect, \"w\", ENCODING) as templateFH:\n",
    "    articlesWriter = csv.writer(articlesFH, quoting=csv.QUOTE_MINIMAL)\n",
    "    redirectWriter = csv.writer(redirectFH, quoting=csv.QUOTE_MINIMAL)\n",
    "    templateWriter = csv.writer(templateFH, quoting=csv.QUOTE_MINIMAL)\n",
    "    articlesWriter.writerow(['id', 'title', 'redirect'])\n",
    "    redirectWriter.writerow(['id', 'title', 'redirect'])\n",
    "    templateWriter.writerow(['id', 'title'])\n",
    "\n",
    "    for event, elem in etree.iterparse(pathWikiXML, events=('start', 'end')):\n",
    "        tname = strip_tag_name(elem.tag)\n",
    "        if event == 'start':\n",
    "            if tname == 'page':\n",
    "                title = ''\n",
    "                id = -1\n",
    "                redirect = ''\n",
    "                inrevision = False\n",
    "                ns = 0\n",
    "            elif tname == 'revision':\n",
    "                # Do not pick up on revision id's\n",
    "                inrevision = True\n",
    "        else:\n",
    "            if tname == 'title':\n",
    "                title = elem.text\n",
    "            elif tname == 'id' and not inrevision:\n",
    "                id = int(elem.text)\n",
    "            elif tname == 'redirect':\n",
    "                redirect = elem.attrib['title']\n",
    "            elif tname == 'ns':\n",
    "                ns = int(elem.text)\n",
    "            elif tname == 'page':\n",
    "                totalCount += 1\n",
    "                if ns == 10:\n",
    "                    templateCount += 1\n",
    "                    templateWriter.writerow([id, title])\n",
    "                elif len(redirect) > 0:\n",
    "                    articleCount += 1\n",
    "                    articlesWriter.writerow([id, title, redirect])\n",
    "                else:\n",
    "                    redirectCount += 1\n",
    "                    redirectWriter.writerow([id, title, redirect])\n",
    "\n",
    "        if totalCount > 1 and (totalCount % 100000) == 0:\n",
    "            print(\"{:,}\".format(totalCount))\n",
    "\n",
    "    elem.clear()\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Total pages: {:,}\".format(totalCount))\n",
    "    print(\"Template pages: {:,}\".format(templateCount))\n",
    "    print(\"Article pages: {:,}\".format(articleCount))\n",
    "    print(\"Redirect pages: {:,}\".format(redirectCount))\n",
    "    print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dbpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T02:30:08.138514Z",
     "start_time": "2017-09-19T02:30:08.081843Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from xml.etree import ElementTree\n",
    "from collections import defaultdict\n",
    "\n",
    "def dbpedia_keyword_search(keywords, api_host='http://localhost:1111', query_class=''):\n",
    "    api_string = api_host+\"/api/search/KeywordSearch?\"\n",
    "    query_class = 'QueryClass=' + query_class + \"&\"\n",
    "    query_string = 'QueryString=' + keywords\n",
    "    request_string = api_string + query_class + query_string\n",
    "    \n",
    "    response = requests.get(request_string)  \n",
    "    xml_tree = ElementTree.fromstring(response.content)\n",
    "    return response.content\n",
    "\n",
    "def dbpedia_prefix_search(query_string, api_host='http://localhost:1111', query_class=''):\n",
    "    '''Returns list of dicts from dbpedia API search. Keys are: label, uri, description'''\n",
    "    api_string = api_host+\"/api/search/PrefixSearch?\"\n",
    "    query_class = 'QueryClass=' + query_class + \"&\"\n",
    "    query_string = 'QueryString=' + query_string\n",
    "    request_string = api_string + query_class + query_string\n",
    "    \n",
    "    response = requests.get(request_string)\n",
    "    xmltree = ElementTree.fromstring(response.content)\n",
    "    \n",
    "    lookup = './/{http://lookup.dbpedia.org/}'\n",
    "    results = xmltree.findall(lookup+\"Result\")\n",
    "    if not results:\n",
    "        return None\n",
    "    \n",
    "    results_list = []\n",
    "    for i, result in enumerate(results):\n",
    "        results_list.append({\n",
    "            'label': xmltree.findall(lookup+\"Label\")[i].text,\n",
    "            'uri'  : xmltree.findall(lookup+\"URI\")[i].text,\n",
    "            'description' : xmltree.findall(lookup+\"Description\")[i].text\n",
    "        })\n",
    "    return results_list\n",
    "\n",
    "print(dbpedia_keyword_search(\"banana\"))\n",
    "print(dbpedia_prefix_search(\"banana\")[0])\n",
    "print(dbpedia_prefix_search(\"machine lea\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:07:45.441434Z",
     "start_time": "2017-09-19T00:07:25.750924Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dbpedia_results(queries):\n",
    "    '''Given a list of query strings, returns a list of result dicts.'''\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        result_for_query = dbpedia_prefix_search(query)\n",
    "        if result_for_query:\n",
    "            results.append(result_for_query[0])\n",
    "    return results\n",
    "\n",
    "#print(get_dbpedia_results(expanded_keywords))\n",
    "\n",
    "def get_dbpedia_result_text(queries):\n",
    "    '''Given a list of query strings, returns a list of (label+description) strings '''\n",
    "    results = get_dbpedia_results(queries)\n",
    "    \n",
    "    results_strings = [str(r['label'] +\": \"+ r['description']) \n",
    "                       for r in results if r['description']]\n",
    "    return list(set(results_strings))\n",
    "\n",
    "db_docs = get_dbpedia_result_text(expanded_keywords)\n",
    "pprint(db_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:23.033570Z",
     "start_time": "2017-09-19T00:07:45.443637Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textacy_corpus_dbpedia_results(queries):\n",
    "    '''Given a list of query strings, returns a textacy corpus generated from dbpedia results.'''\n",
    "    corpus = textacy.corpus.Corpus('en', \n",
    "                                 get_dbpedia_result_text(queries), \n",
    "                                 metadatas=get_dbpedia_results(queries))\n",
    "    return corpus\n",
    "\n",
    "#corpus = textacy_corpus_dbpedia_results(expanded_keywords)\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:23.062858Z",
     "start_time": "2017-09-19T00:07:25.315Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pos_tagged_blob = extract_pos_tagged_sents_from_corpus(corpus)\n",
    "#print(pos_tagged_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-04T21:06:12.275296Z",
     "start_time": "2017-10-04T21:06:11.556251Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-04T21:07:08.023801Z",
     "start_time": "2017-10-04T21:07:07.896864Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def get_semantic_categories(raw_text):\n",
    "    category_analysis = lexicon.analyze(raw_text, normalize=True, tokenizer='default')\n",
    "    top_cats = [[cat[0], cat[1]] for cat in category_analysis.items()]\n",
    "    top_cats.sort(key=lambda x: x[1], reverse=True)\n",
    "    return top_cats\n",
    "\n",
    "input_doc = \"I hated that comment from you. It was useless.\"\n",
    "\n",
    "category_analysis = lexicon.analyze(input_doc, normalize=True, tokenizer='default')\n",
    "\n",
    "top_cats = [[cat[0], cat[1]] for cat in category_analysis.items()]\n",
    "top_cats.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "pprint(top_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Category from terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.070320Z",
     "start_time": "2017-09-19T00:08:41.913900Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = '''machine, word, language, information, human, style, little, thought, vocabulary, contextual'''\n",
    "tokens = tokens.split(\", \")\n",
    "lexicon.create_category(\"category_name\",tokens, model=\"nytimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:09:42.768113Z",
     "start_time": "2017-09-19T00:09:42.274481Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import nltk\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def capture_from_stdout(function):\n",
    "    '''Not 100% sure the interior fuction call syntax is correct, but the wrapping is correct'''\n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        function()\n",
    "    return f.getvalue()\n",
    "\n",
    "def category_from_keywords(keywords, model='all', clean=False):\n",
    "    '''Call Empath's create category. Model options: \"fiction\",\"nytimes\",\"reddit\",\"all\"\n",
    "    Returns a list of strings.'''\n",
    "    \n",
    "    if type(keywords) == str:\n",
    "        keywords = map(str.strip, keywords.split(\",\")) #split into individual items\n",
    "    \n",
    "    #replace spaces with underscores for Empath's lexicon format\n",
    "    keywords = [keyword.replace(\" \", \"_\") for keyword in keywords]\n",
    "\n",
    "    category_name = keywords[0] + \" \" + keywords[1] #name the category after the first two keywords\n",
    "    if model  == 'all':\n",
    "        category_terms = []\n",
    "        for model in ['reddit','nytimes','fiction']:\n",
    "            f = io.StringIO()\n",
    "            with redirect_stdout(f):\n",
    "                lexicon.create_category(category_name, keywords, model=model, write=False)\n",
    "            model_terms = f.getvalue().strip().replace(\"[]\",\"\")\n",
    "            if model_terms:\n",
    "                model_terms = model_terms[1:-1] #exclude enclosing brackets\n",
    "                model_terms = model_terms.replace('\"','').split(\", \")\n",
    "                category_terms.append(model_terms)\n",
    "        category_terms = [term for model_terms in category_terms for term in model_terms] #flatten lists\n",
    "        category_terms = [term.replace('_',' ') for term in category_terms] #re-separate on underscores\n",
    "    \n",
    "    else:\n",
    "        f = io.StringIO()\n",
    "        with redirect_stdout(f):\n",
    "            lexicon.create_category(category_name, keywords, model=model)\n",
    "        category_terms = f.getvalue().strip().replace(\"[]\",\"\").replace(\"_\", \" \")\n",
    "        category_terms = category_terms[1:-1] #exclude enclosing brackets\n",
    "        category_terms = category_terms.replace('\"','').split(\", \")\n",
    "\n",
    "    \n",
    "    ### Filter out non-words like urls, unusual characters\n",
    "    if clean == True:\n",
    "        clean_terms = []\n",
    "        for term in category_terms:\n",
    "            clean_terms.append(\" \".join(w for w in nltk.wordpunct_tokenize(term) \\\n",
    "             if w.lower() in words or not w.isalpha()))\n",
    "        category_terms = [term for term in clean_terms if term.isalpha()]\n",
    "    \n",
    "    return category_terms\n",
    "\n",
    "keywords = '''machine, word, language, information, human, style, little, thought, vocabulary, contextual'''\n",
    "expanded_keywords = category_from_keywords(keywords, model='fiction')\n",
    "print(keywords)\n",
    "print(expanded_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.074280Z",
     "start_time": "2017-09-19T00:08:41.878Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wiki_text_for_keywords(keywords):\n",
    "    all_text = \"\"\n",
    "    if type(keywords)==str:\n",
    "        keywords = keywords.split(\",\")\n",
    "        \n",
    "    print(keywords)    \n",
    "    for keyword in keywords:\n",
    "        wiki_data = get_wiki_page(keyword, summary=True, content=True)\n",
    "        if wiki_data != None:\n",
    "            try:\n",
    "                #summary = wiki_data['summary']\n",
    "                content = wiki_data['content']\n",
    "                all_text += content\n",
    "            except KeyError:\n",
    "                continue\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "wiki_blob = get_wiki_text_for_keywords(keywords)\n",
    "print(wiki_blob)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Category from Key Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.075686Z",
     "start_time": "2017-09-19T00:08:41.897Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_abstract = '''This research looks at the work of Margaret C. Anderson, the editor of the Little Review.  The review published first works by Sherwood Anderson, James Joyce, Wyndham Lewis, and Ezra Pound.  This research draws upon mostly primary sources including memoirs, published letters, and a complete collection of the Little Review. Most prior research on Anderson focuses on her connection to the famous writers and personalities that she published and associated with.  This focus undermines her role as the dominant creative force behind one of the most influential little magazines published in the 20th Century. This case example shows how little magazine publishing is arguably a literary art.'''\n",
    "keyterms = get_textacy_key_terms(to_textacy_doc(sample_abstract))\n",
    "print(\"Extracted Key Terms:\\n\", keyterms)\n",
    "sample_abstract_cat = category_from_keywords([keyterm[0] for keyterm in keyterms])\n",
    "print(\"Generated Category Words:\\n\", sample_abstract_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T17:59:10.873708Z",
     "start_time": "2017-09-12T17:59:10.866202Z"
    }
   },
   "source": [
    "## Markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T01:31:27.325972Z",
     "start_time": "2017-09-19T01:31:27.106614Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import markovify\n",
    "text = raw_abstracts[1]\n",
    "\n",
    "# Build the model.\n",
    "text_model = markovify.Text(text)\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "for i in range(5):\n",
    "    print(text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tagger class using spaCy components for fast POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.078417Z",
     "start_time": "2017-09-19T00:08:41.928Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import markovify\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "# corpus_path = \"/corpus_data/comp_ling.txt\"\n",
    "# corpus_name = \"comp_ling\"\n",
    "#corpus = textacy.Corpus('en', [raw_abstracts[1], \" \"])\n",
    "#corpus = textacy.Corpus.add_doc(textacy.Corpus('en'), to_textacy_doc(wiki_blob))\n",
    "#corpus = wiki_blob_spacy\n",
    "#print(corpus)\n",
    "\n",
    "class TaggedText(markovify.Text):\n",
    "\n",
    "    def sentence_split(self, text):\n",
    "        \"\"\"\n",
    "        Splits full-text string into a list of sentences.\n",
    "        \"\"\"\n",
    "        sentence_list = []\n",
    "        for doc in corpus:\n",
    "            sentence_list += list(doc.sents)\n",
    "\n",
    "        return sentence_list\n",
    "\n",
    "    def word_split(self, sentence):\n",
    "        \"\"\"\n",
    "        Splits a sentence into a list of words.\n",
    "        \"\"\"\n",
    "        #print(sentence)\n",
    "        return [\"::\".join((word.orth_,word.pos_)) for word in sentence]\n",
    "\n",
    "    def word_join(self, words):\n",
    "        sentence = \" \".join(word.split(\"::\")[0] for word in words)\n",
    "        return sentence\n",
    "\n",
    "    def test_sentence_input(self, sentence):\n",
    "        \"\"\"\n",
    "        A basic sentence filter. This one rejects sentences that contain\n",
    "        the type of punctuation that would look strange on its own\n",
    "        in a randomly-generated sentence. \n",
    "        \"\"\"\n",
    "        sentence = sentence.text\n",
    "        reject_pat = re.compile(r\"(^')|('$)|\\s'|'\\s|[\\\"(\\(\\)\\[\\])]\")\n",
    "        # Decode unicode, mainly to normalize fancy quotation marks\n",
    "        if sentence.__class__.__name__ == \"str\":\n",
    "            decoded = sentence\n",
    "        else:\n",
    "            decoded = unidecode(sentence)\n",
    "        # Sentence shouldn't contain problematic characters\n",
    "        if re.search(reject_pat, decoded): return False\n",
    "        return True\n",
    "\n",
    "    def generate_corpus(self, text):\n",
    "        \"\"\"\n",
    "        Given a text string, returns a list of lists; that is, a list of\n",
    "        \"sentences,\" each of which is a list of words. Before splitting into \n",
    "        words, the sentences are filtered through `self.test_sentence_input`\n",
    "        \"\"\"\n",
    "        sentences = self.sentence_split(text)\n",
    "        passing = filter(self.test_sentence_input, sentences)\n",
    "        runs = list(map(self.word_split, passing))\n",
    "        #print(runs[:10])\n",
    "        return runs\n",
    "\n",
    "# Generated the model\n",
    "model = TaggedText(corpus, state_size=2)\n",
    "# A sentence based on the model\n",
    "print(model.make_sentence())\n",
    "model.make_short_sentence(max_chars=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Sentence Starting With..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.080022Z",
     "start_time": "2017-09-19T00:08:41.941Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Using default Markovify model\n",
    "text = raw_abstracts[1]\n",
    "text_model = markovify.Text(text, state_size=1)\n",
    "try:\n",
    "    for i in range(5):\n",
    "        print(text_model.make_sentence_with_start(\"importance\"))\n",
    "except KeyError:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.081685Z",
     "start_time": "2017-09-19T00:08:41.958Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Using spaCy-fied POS parsed model\n",
    "model = TaggedText(corpus, state_size=1)\n",
    "try:\n",
    "    start_with_token = \"importance\"\n",
    "    for i in range(5):\n",
    "        print(model.make_sentence_with_start(nlp(start_with_token)))\n",
    "except KeyError:\n",
    "    None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.083216Z",
     "start_time": "2017-09-19T00:08:41.971Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_markov_completions(partial_sentence, trained_markovify_model, n_completions=10):\n",
    "    spacy_sentence = nlp(partial_sentence) #convert to spaCy doc\n",
    "    last_word = str(spacy_sentence[-1])\n",
    "    #last_words = str(spacy_sentence[-2]) + \" \" + last_word\n",
    "\n",
    "    completions = []\n",
    "    for completion in range(n_completions):\n",
    "        completions.append(trained_markovify_model.make_sentence_with_start(last_word))\n",
    "        \n",
    "    return list(set(completions))\n",
    "\n",
    "try:\n",
    "    generate_markov_completions(\"Overwatch\", model)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T18:01:23.594143Z",
     "start_time": "2017-09-12T18:01:23.591123Z"
    }
   },
   "source": [
    "## Synonym Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Wordnet Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.088577Z",
     "start_time": "2017-09-19T00:08:41.983Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "for i,j in enumerate(wn.synsets('java')):\n",
    "    print(\"Meaning\",i, \"NLTK ID:\", j.name())\n",
    "    print(\"Definition:\",j.definition())\n",
    "    print(\"Synonyms:\", \", \".join(j.lemma_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.091269Z",
     "start_time": "2017-09-19T00:08:41.999Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for ss in wn.synsets('small'):\n",
    "    print(ss, ss.examples())\n",
    "    for sim in ss.similar_tos():\n",
    "        print('    {}'.format(sim))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.092926Z",
     "start_time": "2017-09-19T00:08:42.013Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "for i,j in enumerate(wn.synsets('computer')):\n",
    "    print(\"Meaning\",i, \"NLTK ID:\", j.name())\n",
    "    hypernyms = list(chain(*[l.lemma_names() for l in j.hypernyms()]))\n",
    "    hyponyms = list(chain(*[l.lemma_names() for l in j.hyponyms()]))\n",
    "    print(\"Hypernyms:\", \", \".join(hypernyms))\n",
    "    print(\"Hyponyms:\", \", \".join(hyponyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.093631Z",
     "start_time": "2017-09-19T00:08:42.028Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "def dict_from_doc_tokens(unigram_tokens):\n",
    "    '''Calls PyDictionary (which calls thesaurus.com) to retrieve synonyms'''\n",
    "    pydict=PyDictionary(unigram_tokens)\n",
    "    \n",
    "    dictionary = defaultdict(dict)\n",
    "    for word in unigram_tokens:\n",
    "        meaning = pydict.meaning(word)\n",
    "        synonyms = pydict.synonym(word)\n",
    "        dictionary[word] = {\"meaning\":meaning, \"synonyms\":synonyms}\n",
    "    return dictionary\n",
    "\n",
    "def get_meanings(word, pos=\"all\"):\n",
    "    '''Returns meaning definitions from an existing pydictionary, or None if no meanings found'''\n",
    "    meanings = dictionary[word]['meaning']\n",
    "\n",
    "    if pos.startswith(\"all\"):\n",
    "        return meanings\n",
    "    if pos.startswith(\"N\"):\n",
    "        return meanings['Noun']\n",
    "    if pos.startswith(\"V\"):\n",
    "        return meanings[\"Verb\"]\n",
    "    if pos.startswith(\"J\"):\n",
    "        return meanings[\"Adjective\"]\n",
    "    if pos.startswith(\"RB\"):\n",
    "        return meanings[\"Adverb\"]\n",
    "    \n",
    "    return meanings\n",
    "\n",
    "tokens = ['alabaster']\n",
    "dictionary = dict_from_doc_tokens(tokens)\n",
    "pprint(get_meanings(tokens[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Proselint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.094388Z",
     "start_time": "2017-09-19T00:08:42.047Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import proselint\n",
    "from proselint.tools import errors_to_json\n",
    "\n",
    "suggestions = proselint.tools.lint(\"this is a very unique sentence\")\n",
    "errors_to_json(suggestions)\n",
    "\n",
    "# for suggestion in suggestions:\n",
    "#         check = suggestion[0]\n",
    "#         message = suggestion[1]\n",
    "#         line = suggestion[2]\n",
    "#         column = suggestion[3]\n",
    "#         start = suggestion[4]\n",
    "#         end = suggestion[5]\n",
    "#         extent = suggestion[6]\n",
    "#         severity = suggestion[7]\n",
    "#         replacements = suggestion[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.097651Z",
     "start_time": "2017-09-19T00:08:42.062Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import proselint\n",
    "from proselint.tools import errors_to_json\n",
    "import json\n",
    "\n",
    "def linter_suggestions(text):\n",
    "    ''' Returns suggestions as a list of dicts. Each dict is a suggestion with the following properties:\n",
    "    (check, message, line, column, start, end, extent, severity, replacements)\n",
    "    '''\n",
    "    suggestions = proselint.tools.lint(text)\n",
    "    json_string = errors_to_json(suggestions) \n",
    "    json_dict = json.loads(json_string)\n",
    "    return json_dict['data']['errors']\n",
    "\n",
    "linter_suggestions(\"and then I said... there goes a very unique thing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T02:44:08.406450Z",
     "start_time": "2017-09-12T02:44:08.345351Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## TextGenRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.100571Z",
     "start_time": "2017-09-19T00:08:42.081Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from textgenrnn import textgenrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.122174Z",
     "start_time": "2017-09-19T00:08:42.109Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "textgen = textgenrnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.134544Z",
     "start_time": "2017-09-19T00:08:42.126Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "textgen.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.144716Z",
     "start_time": "2017-09-19T00:08:42.143Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "generated_texts = textgen.generate(n=5, prefix=\"Machine learning\", temperature=0.2, return_as_list=True)\n",
    "pprint(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.158071Z",
     "start_time": "2017-09-19T00:08:42.156Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts = [input_doc]\n",
    "\n",
    "textgen.train_on_texts(texts, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.170893Z",
     "start_time": "2017-09-19T00:08:42.167Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"---\\nNormal Output\\n---\")\n",
    "textgen.generate(5)\n",
    "print(\"---\\nHigh Temperature Output\\n---\")\n",
    "textgen.generate(5, temperature=1.0)\n",
    "print(\"---\\nPrefix Output\\n---\")\n",
    "textgen.generate(5, prefix=\"N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.189490Z",
     "start_time": "2017-09-19T00:08:42.187Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.200840Z",
     "start_time": "2017-09-19T00:08:42.198Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_summary(text, ratio=0.25):\n",
    "    '''Wraps gensim summarize()'''\n",
    "    return summarize(text, ratio)\n",
    "extract_summary(input_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.213944Z",
     "start_time": "2017-09-19T00:08:42.212Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "keywords(input_doc).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-18T23:45:24.408292Z",
     "start_time": "2017-09-18T23:45:24.310704Z"
    }
   },
   "source": [
    "## Sentence completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.229138Z",
     "start_time": "2017-09-19T00:08:42.227Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sentence Sources\n",
    "#dbpedia\n",
    "#PyDictionary (most flexible, but makes web call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.243593Z",
     "start_time": "2017-09-19T00:08:42.241Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def extract_keywords(text):\n",
    "#    '''Wraps textacy keywords function, returns a list of keyword strings'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.257450Z",
     "start_time": "2017-09-19T00:08:42.255Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def category_from_keywords(keywords, model='all', clean=False):\n",
    "#'''Call Empath's create category. Model options: \"fiction\",\"nytimes\",\"reddit\",\"all\"\n",
    "#    Returns a list of strings.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.277626Z",
     "start_time": "2017-09-19T00:08:42.275Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def textacy_corpus_dbpedia_results(queries):\n",
    "#    '''Given a list of query strings, returns a textacy corpus generated from dbpedia results.'''\n",
    "#    corpus = textacy.corpus.Corpus('en', \n",
    "#                                 get_dbpedia_result_text(queries), \n",
    "#                                 metadatas=get_dbpedia_results(queries))\n",
    "#    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:08:42.292413Z",
     "start_time": "2017-09-19T00:08:42.290Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_sentences(doc):\n",
    "#     '''Returns a list of spacy spans.'''\n",
    "#     if not isinstance(doc, textacy.doc.Doc):\n",
    "#         doc = to_textacy_doc(doc)\n",
    "#     return list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T00:56:00.259549Z",
     "start_time": "2017-09-19T00:56:00.254023Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dbpedia_result_text(queries):\n",
    "    '''Given a list of query strings, returns a list of (label+description) strings '''\n",
    "    results = get_dbpedia_results(queries)\n",
    "    \n",
    "    results_strings = [str(r['label'] +\": \"+ r['description']) \n",
    "                       for r in results if r['description']]\n",
    "    return list(set(results_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T02:42:22.087955Z",
     "start_time": "2017-09-19T02:42:22.083815Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = '''Metis is a cool data science bootcamp immersive program where I learned a ton about machine learning, natural language processing, probability, and statistics. Natural Language Processing is cool. Data Science is a burgeoning field. President Obama is cool.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T02:43:59.616232Z",
     "start_time": "2017-09-19T02:43:59.159613Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "doc = to_textacy_doc(text)\n",
    "\n",
    "keywords = get_semantic_key_terms(doc)\n",
    "print(keywords)\n",
    "\n",
    "entities = [str(ent) for ent in textacy.extract.named_entities(doc)]\n",
    "print(entities)\n",
    "\n",
    "knowledge = textacy_corpus_dbpedia_results(keywords+entities)\n",
    "print(knowledge)\n",
    "def get_completions(doc):\n",
    "    '''Accepts string or textacy doc. Returns list of strings.'''\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    \n",
    "    ents = [str(ent) for ent in textacy.extract.named_entities(doc)]   \n",
    "    completions = get_dbpedia_result_text(ents)    \n",
    "        \n",
    "        \n",
    "        \n",
    "    return completions\n",
    "\n",
    "get_completions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T02:35:18.424537Z",
     "start_time": "2017-09-19T02:35:18.395585Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_dbpedia_result_text([\"Natural Language Processing\", \"dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T02:48:21.695035Z",
     "start_time": "2017-09-19T02:48:21.685374Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(textacy.extract.subject_verb_object_triples(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyGal Charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-20T21:52:46.814692Z",
     "start_time": "2017-09-20T21:52:46.246414Z"
    }
   },
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()\n",
    "\n",
    "def empath_analyze(text):\n",
    "    '''Takes in raw text returns list of lists: [[category_name, weight],...]\n",
    "    sorted by weight, descending'''\n",
    "    empath_analysis = lexicon.analyze(text, normalize=True, tokenizer='default')\n",
    "\n",
    "    lexical_categories = [[cat[0], cat[1]] for cat in empath_analysis.items()]\n",
    "    lexical_categories.sort(key=lambda x: x[1], reverse=True)\n",
    "    return lexical_categories\n",
    "\n",
    "#pprint(top_cats)\n",
    "#top_eight = empath_analyze(input_doc)[:8]\n",
    "#pprint(top_eight)\n",
    "make_radar(empath_analyze(input_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-20T21:52:51.158945Z",
     "start_time": "2017-09-20T21:52:51.100386Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pygal                                                       # First import pygal\n",
    "#bar_chart = pygal.Bar()                                            # Then create a bar graph object\n",
    "#bar_chart.add('Fibonacci', [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55])  # Add some values\n",
    "#bar_chart.render_to_file('static/bar_chart.svg')                          # Save the svg to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T23:24:15.891458Z",
     "start_time": "2017-09-19T23:24:15.772941Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_radar(lex_cats):\n",
    "    '''Takes a list of lists e.g. [['fruit',0.02],['science',0.01]]\n",
    "       Creates an svg file called lexical_radar.svg.'''\n",
    "    radar_chart = pygal.Radar()\n",
    "    radar_chart.title = 'Top 8 Lexical Categories by Word Count'\n",
    "    lex_cats.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_eight = lex_cats[:8]\n",
    "    radar_chart.x_labels = [cat[0] for cat in top_eight[::-1]]\n",
    "    radar_chart.add('doc', [cat[1] for cat in top_eight[::-1]])\n",
    "    radar_chart.render_to_file('lexical_radar.svg', fill=True) \n",
    "    return True\n",
    "    \n",
    "make_radar(top_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-20T21:59:50.150370Z",
     "start_time": "2017-09-20T21:59:50.112303Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_hist(lex_cats):\n",
    "    '''Takes a list of lists e.g. [['fruit',0.02],['science',0.01]]\n",
    "       Creates an svg file called lexical_histogram.svg.'''\n",
    "    lex_cats.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_eight = lex_cats[:8]\n",
    "    \n",
    "    hist = pygal.Histogram()\n",
    "    for i, cat in enumerate(top_eight):\n",
    "        #hist.add(name, [(height, start, stop)])\n",
    "        hist.add(cat[0], [(cat[1],i+1,i+2)])\n",
    "    hist.render_to_file('lexical_histogram.svg')\n",
    "    return True\n",
    "make_hist(empath_analyze('punch drunk love'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T06:46:43.912157Z",
     "start_time": "2017-09-19T06:46:43.857574Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_readability_gauge(doc):\n",
    "    '''Takes a document and creates an svg file called reada_gauge'''\n",
    "    gauge = pygal.Gauge()\n",
    "    gauge.title = 'Flesch Reading Ease Score'\n",
    "    gauge.range = [0, 100]\n",
    "    gauge.add('Your writing', get_readability_stats(doc)['flesch_readability_ease'])\n",
    "    gauge.add('Very easy to read', 100)\n",
    "    gauge.add('Average readability', 65)\n",
    "    gauge.add('A little hard to read', 35)\n",
    "    gauge.add('Very hard to read', 0)\n",
    "    gauge.render_to_file('reada_gauge.svg', needle_width = 1 / 10, human_readable=True)\n",
    "    return\n",
    "make_readability_gauge(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-19T06:34:35.130116Z",
     "start_time": "2017-09-19T06:34:35.097291Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
